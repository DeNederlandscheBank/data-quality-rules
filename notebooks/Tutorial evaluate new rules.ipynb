{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial describes how to evaluate new rules applicable to the derivatives data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arelle import ModelManager, Cntlr, ModelFormulaObject, ModelXbrl, ViewFileFormulae, XbrlConst, ViewFileRenderedGrid\n",
    "from arelle import RenderingEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import re\n",
    "from src import Evaluator\n",
    "import logging\n",
    "import data_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECIMALS = 0\n",
    "RULES_PATH = join('..', 'solvency2-rules')\n",
    "INSTANCES_DATA_PATH = join('..', 'tests', 'data')\n",
    "FILENAME_DATAPOINTS = 'QRS.csv'\n",
    "RESULTS_PATH = join('..', 'results')\n",
    "DATA_PATH = join('..', 'data')\n",
    "logging.basicConfig(filename = join(RESULTS_PATH, 'rules.log'),level = logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing the new rules applicable to the derivatives data. One set of rules is applicable to template S2.08.01.01 (Open derivatives). This template consists of both S2.08.01.01.01 (Information on positions held) and S2.08.01.01.02 (Information on derivatives). The other set of rules is only applicable to S2.08.01.01.02 (Information on derivatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = pd.read_excel(join(RULES_PATH,'S2_08_01_01.xlsx'), index_col='index')\n",
    "dfr2 = pd.read_excel(join(RULES_PATH,'S2_08_01_01_02.xlsx'), index_col='index')\n",
    "\n",
    "#Capitalize row-column references:\n",
    "column_replace = set([column for sublist in [row for row in dfr['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)\n",
    "column_replace = set([column for sublist in [row for row in dfr2['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr2.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the reporting data. In the tutorial 'Convert Solvency 2 XBRL-instances to CSV, HTML and pickles' the XBRL-instances are converted to pickle files per template. These pickle files are written to the data/instances folder.  We are going to import these pickle files. In this tutorial we use dummy pickle files that are in the tests/data folder of this repository. \n",
    "\n",
    "We create two dataframes with reporting data. For the set of rules that is applicable to template S2.08.01.01 (Open derivatives) we merge S2.08.01.01.01 (Information on positions held) and S2.08.01.01.02 (Information on derivatives). The other set of rules is only applicable to S2.08.01.01.02 (Information on derivatives).\n",
    "\n",
    "For the sake of simplicity we only import the Quarterly Solvency II reporting Solo (QRS) templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(join(INSTANCES_DATA_PATH,'S.08.01.01.01.pickle')).reset_index()\n",
    "df2 = pd.read_pickle(join(INSTANCES_DATA_PATH, 'S.08.01.01.02.pickle')).reset_index()\n",
    "df = pd.merge(df1,df2,how='inner', left_on=['entity','period','S.08.01.01.01,C0040'], right_on=['entity','period','S.08.01.01.02,C0040']).set_index(['entity', 'period', 'S.08.01.01.01,C0040'])\n",
    "df2 = df2.set_index(['entity', 'period', 'S.08.01.01.02,C0040'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate the rules. First, we construct a PatternMiner-object with the data-patterns package using the rules dataframe. Second, we use the analyze-function to get the results of the rules. We do this for both set of rules separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr)\n",
    "results = miner.analyze(df)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr2)\n",
    "results2 = miner.analyze(df2)\n",
    "results2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
