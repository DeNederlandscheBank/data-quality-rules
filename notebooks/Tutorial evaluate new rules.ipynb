{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial describes how to evaluate new rules applicable to the assets and derivatives data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arelle import ModelManager, Cntlr, ModelFormulaObject, ModelXbrl, ViewFileFormulae, XbrlConst, ViewFileRenderedGrid\n",
    "from arelle import RenderingEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import re\n",
    "from src import Evaluator\n",
    "import logging\n",
    "import data_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECIMALS = 0\n",
    "RULES_PATH = join('..', 'solvency2-rules')\n",
    "INSTANCES_DATA_PATH = join('..', 'data', 'instances', '...') #path of folder with converted xbrl-instance data\n",
    "TEST_DATA_PATH = join('..', 'tests', 'data', 'demo') #path of folder with demo data\n",
    "RESULTS_PATH = join('..', 'results')\n",
    "DATA_PATH = join('..', 'data')\n",
    "logging.basicConfig(filename = join(RESULTS_PATH, 'rules.log'),level = logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing the new rules applicable to the assets and derivatives data. There are several sets of rules applicable to different templates:\n",
    "* S.06.02.01 (Information on positions held)\n",
    "* S.06.02.02 (Information on assets)\n",
    "* S.06.02.01 (Information on positions held) and S.06.02.02 (Information on assets)\n",
    "* S.08.01.01.01 (Information on positions held) and S2.08.01.01.02 (Information on derivatives)\n",
    "* S.08.01.01.02 (Information on derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_s06 = pd.read_excel(join(RULES_PATH,'S2_06_02.xlsx'), engine='openpyxl')\n",
    "dfr_s06_2 = pd.read_excel(join(RULES_PATH,'S2_06_02_01_02.xlsx'), engine='openpyxl')\n",
    "dfr_s06_1 = pd.read_excel(join(RULES_PATH,'S2_06_02_01_01.xlsx'), engine='openpyxl')\n",
    "\n",
    "#Capitalize row-column references:\n",
    "column_replace = set([column for sublist in [row for row in dfr_s06['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_s06.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)\n",
    "column_replace = set([column for sublist in [row for row in dfr_s06_2['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_s06_2.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)\n",
    "column_replace = set([column for sublist in [row for row in dfr_s06_1['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_s06_1.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_s08 = pd.read_excel(join(RULES_PATH,'S2_08_01_01.xlsx'), engine='openpyxl')\n",
    "dfr_s08_2 = pd.read_excel(join(RULES_PATH,'S2_08_01_01_02.xlsx'), engine='openpyxl')\n",
    "\n",
    "#Capitalize row-column references:\n",
    "column_replace = set([column for sublist in [row for row in dfr_s08['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_s08.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)\n",
    "column_replace = set([column for sublist in [row for row in dfr_s08_2['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_s08_2.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the reporting data. In the tutorial 'Convert XBRL-instances to CSV, HTML and pickles' the XBRL-instances are converted to pickle files per template. The pickle files are written to the data/instances folder. We import these pickle files. We merge dataframes for the sets of rules that are applicable to two templates. For the sake of simplicity we only import the Quarterly Solvency II reporting Solo (QRS) templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and make index unique if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_s06_1 = pd.read_pickle(join(INSTANCES_DATA_PATH,'S.06.02.01.01.pickle')).fillna(0).reset_index()\n",
    "# df_s06_1['S.06.02.01.01,C0040A'] = df_s06_1['S.06.02.01.01,C0040']\n",
    "# listt=list(df_s06_1['S.06.02.01.01,C0040A'])\n",
    "# for i in listt:\n",
    "#     lenn = len(df_s06_1[df_s06_1['S.06.02.01.01,C0040A']==i])\n",
    "#     if lenn > 1:\n",
    "#         list_ind = list(df_s06_1.loc[df_s06_1['S.06.02.01.01,C0040A']==i].index)\n",
    "#         temp = 0\n",
    "#         for j in list_ind[1:]:\n",
    "#             temp=temp+1\n",
    "#             df_s06_1['S.06.02.01.01,C0040A'].iloc[j] = df_s06_1['S.06.02.01.01,C0040A'].iloc[j] + '_' + str(temp)\n",
    "# df_s06_1 = df_s06_1.set_index(['entity', 'period', 'S.06.02.01.01,C0040A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_s06_2 = pd.read_pickle(join(INSTANCES_DATA_PATH, 'S.06.02.01.02.pickle')).fillna(0).reset_index()\n",
    "# df_s06_2 = df_s06_2.set_index(['entity', 'period', 'S.06.02.01.02,C0040'])\n",
    "# df_s06_2['S.06.02.01.02,C0040'] = df_s06_2.index.get_level_values(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_s06 = pd.merge(pd.read_pickle(join(INSTANCES_DATA_PATH,'S.06.02.01.01.pickle')).reset_index(),pd.read_pickle(join(INSTANCES_DATA_PATH, 'S.06.02.01.02.pickle')).reset_index(),how='inner', left_on=['entity','period','S.06.02.01.01,C0040'], right_on=['entity','period','S.06.02.01.02,C0040']).set_index(['entity', 'period', 'S.06.02.01.01,C0040'])\n",
    "# df_s06 = df_s06.fillna(0).reset_index()\n",
    "# df_s06['S.06.02.01.02,C0040A'] = df_s06['S.06.02.01.02,C0040']\n",
    "# listt=list(df_s06['S.06.02.01.02,C0040A'])\n",
    "# for i in listt:\n",
    "#     lenn = len(df_s06[df_s06['S.06.02.01.02,C0040A']==i])\n",
    "#     if lenn > 1:\n",
    "#         list_ind = list(df_s06.loc[df_s06['S.06.02.01.02,C0040A']==i].index)\n",
    "#         temp = 0\n",
    "#         for j in list_ind[1:]:\n",
    "#             temp=temp+1\n",
    "#             df_s06['S.06.02.01.02,C0040A'].iloc[j] = df_s06['S.06.02.01.02,C0040A'].iloc[j] + '_' + str(temp)\n",
    "# df_s06 = df_s06.set_index(['entity', 'period', 'S.06.02.01.02,C0040A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we work with dummy data in order to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s06_1 = pd.read_pickle(join(TEST_DATA_PATH,'S.06.02.01.01.pickle')).reset_index() #Import demo pickles\n",
    "df_s06_2 = pd.read_pickle(join(TEST_DATA_PATH,'S.06.02.01.02.pickle')).reset_index() #Import demo pickles\n",
    "df_s06 = pd.merge(df_s06_1,df_s06_2,how='inner', left_on=['entity','period','S.06.02.01.01,C0040'], right_on=['entity','period','S.06.02.01.02,C0040']).set_index(['entity', 'period', 'S.06.02.01.01,C0040'])\n",
    "df_s06_2 = df_s06_2.set_index(['entity', 'period', 'S.06.02.01.02,C0040'])\n",
    "df_s06_1 = df_s06_1.set_index(['entity', 'period', 'S.06.02.01.01,C0040'])\n",
    "df_s06_2['S.06.02.01.02,C0040'] = df_s06_2.index.get_level_values(2)\n",
    "df_s06['S.06.02.01.01,C0040'] = df_s06.index.get_level_values(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and make index unique if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_s08_2 = pd.read_pickle(join(INSTANCES_DATA_PATH, 'S.08.01.01.02.pickle')).fillna(0).reset_index()\n",
    "# df_s08_2 = df_s08_2.set_index(['entity', 'period', 'S.08.01.01.02,C0040'])\n",
    "# df_s08_2['S.08.01.01.02,C0040'] = df_s08_2.index.get_level_values(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_s08 = pd.merge(pd.read_pickle(join(INSTANCES_DATA_PATH,'S.08.01.01.01.pickle')).reset_index(),pd.read_pickle(join(INSTANCES_DATA_PATH, 'S.08.01.01.02.pickle')).reset_index(),how='inner', left_on=['entity','period','S.08.01.01.01,C0040'], right_on=['entity','period','S.08.01.01.02,C0040']).set_index(['entity', 'period', 'S.08.01.01.01,C0040'])\n",
    "# df_s08 = df_s08.fillna(0).reset_index()\n",
    "# df_s08['S.08.01.01.02,C0040A'] = df_s08['S.08.01.01.02,C0040']\n",
    "# listt=list(df_s08['S.08.01.01.02,C0040A'])\n",
    "# for i in listt:\n",
    "#     lenn = len(df_s08[df_s08['S.08.01.01.02,C0040A']==i])\n",
    "#     if lenn > 1:\n",
    "#         list_ind = list(df_s08.loc[df_s08['S.08.01.01.02,C0040A']==i].index)\n",
    "#         temp = 0\n",
    "#         for j in list_ind[1:]:\n",
    "#             temp=temp+1\n",
    "#             df_s08['S.08.01.01.02,C0040A'].iloc[j] = df_s08['S.08.01.01.02,C0040A'].iloc[j] + '_' + str(temp)\n",
    "# df_s08 = df_s08.set_index(['entity', 'period', 'S.08.01.01.02,C0040A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we work with dummy data in order to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s08_1 = pd.read_pickle(join(TEST_DATA_PATH,'S.08.01.01.01.pickle')).reset_index() #Import demo pickles\n",
    "df_s08_2 = pd.read_pickle(join(TEST_DATA_PATH, 'S.08.01.01.02.pickle')).reset_index() #Import demo pickles\n",
    "df_s08 = pd.merge(df_s08_1,df_s08_2,how='inner', left_on=['entity','period','S.08.01.01.01,C0040'], right_on=['entity','period','S.08.01.01.02,C0040']).set_index(['entity', 'period', 'S.08.01.01.01,C0040'])\n",
    "df_s08_2 = df_s08_2.set_index(['entity', 'period', 'S.08.01.01.02,C0040'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate the different sets of rules. First, we construct a PatternMiner-object with the data-patterns package using the rules dataframe. Second, we use the analyze-function to get the results of the rules. We do this for each set of rules separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_s06)\n",
    "results_06 = miner.analyze(df_s06)\n",
    "results_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_s06_2)\n",
    "results_06_2 = miner.analyze(df_s06_2)\n",
    "results_06_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_s06_1)\n",
    "results_06_1 = miner.analyze(df_s06_1)\n",
    "results_06_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_s08)\n",
    "results_08 = miner.analyze(df_s08)\n",
    "results_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner2 = data_patterns.PatternMiner(df_patterns=dfr_s08_2)\n",
    "results_08_2 = miner2.analyze(df_s08_2)\n",
    "results_08_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
