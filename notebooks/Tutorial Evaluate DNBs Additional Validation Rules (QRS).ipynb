{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Evaluate DNBs additional Rules (QRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a tutorial for the evaluation of DNBs additional Rules for the quarterly Solvency II reports for solo entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arelle import ModelManager, Cntlr, ModelFormulaObject, ModelXbrl, ViewFileFormulae, XbrlConst, ViewFileRenderedGrid\n",
    "from arelle import RenderingEvaluator\n",
    "\n",
    "import pandas as pd  # dataframes\n",
    "import numpy as np  # mathematical functions, arrays and matrices\n",
    "from os.path import join, isfile  # some os dependent functionality\n",
    "import re  # regular expressions\n",
    "from src import Evaluator  # conversion from 'rules' to expressions for the data-patterns packages\n",
    "import data_patterns  # evaluation of patterns\n",
    "from pprint import pprint  # pretty print\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the source file with the additional rules\n",
    "RULES_PATH = join('..', 'solvency2-rules')  \n",
    "# path to the source data\n",
    "INSTANCES_DATA_PATH = join('..', 'data', 'instances', 'all')\n",
    "# path to the results\n",
    "RESULTS_PATH = join('..', 'results') \n",
    "# input parameters\n",
    "PARAMETERS = {'decimal': 0}  \n",
    "# currently only 'decimal' is availabl,e which specifies tolerance during evaluation of patterns.\n",
    "# decimal: 0 means tolerance = abs(1.5e-0) (= 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We log to rules.log in the data/instances path\n",
    "logging.basicConfig(filename = join(INSTANCES_DATA_PATH, 'rules.log'),level = logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file with all possible datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simplified taxonomy with all possible datapoints, located in the data/datapoints directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPOINTS_PATH = join('..', 'data', 'datapoints')  # path to file\n",
    "FILENAME_DATAPOINTS = 'QRS.csv'  # filename\n",
    "df_datapoints = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS), sep=\";\")  # load file to dataframe\n",
    "df_datapoints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns with only R(ow) and C(olumn) reference:\n",
    "df_datapoints['rij'] = df_datapoints['datapunt'].apply(lambda x: \"\" if re.search(\"R\\d\\d\\d\\d\", x) is None else re.search(\"R\\d\\d\\d\\d\", x)[0])\n",
    "df_datapoints['kolom'] = df_datapoints['datapunt'].apply(lambda x: \"\" if re.search(\"C\\d\\d\\d\\d\", x) is None else re.search(\"C\\d\\d\\d\\d\", x)[0])\n",
    "df_datapoints.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish 2 types of tables: with a closed axis, and with an open axis.\n",
    "\n",
    "An example of a table with an open axis is the list of assets: an entity reports several 'rows of data' in the relevant table. An example of a closed axis is the balance sheet: an entity reports only 1 balance sheet per period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tables from source path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine all tables with closed axes into one DataFrame. This DataFrame is then used for all validation rules for closed axes tables. \n",
    "\n",
    "Tables with an open axis are put in a dictionary of DataFrames. We perform validation rules per tables with an open axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_complete_set = df_datapoints.tabelcode.sort_values().unique().tolist()  # list of all QRS tables\n",
    "tables = [table for table in tables_complete_set\n",
    "          if isfile(join(INSTANCES_DATA_PATH, table + '.pickle'))]  # QRS tables found in the input folder\n",
    "tables_closed_axis = []  # for listing all input tables with closed axis\n",
    "tables_open_axis = []  # for listing all input tables with open axis\n",
    "df_closed_axis = pd.DataFrame()  # one dataframe with all data from closed axis tables\n",
    "dict_open_axis = {}  # dictionary with all open axis tables\n",
    "\n",
    "for table in tables:\n",
    "    df = pd.read_pickle(join(INSTANCES_DATA_PATH, table + '.pickle'))  # read dataframe\n",
    "    \n",
    "    if df.index.nlevels > 2:  # if more than 2 indexes (entity, period) --> open axis\n",
    "        # Identify which columns within an open axis table make a 'table row' unique (index-columns):\n",
    "        index_columns_open_axis = [col for col in list(df.index.names) if col not in ['entity','period']]\n",
    "        \n",
    "        # Duplicate index-columns to data columns:\n",
    "        df.reset_index(level=index_columns_open_axis, inplace=True)\n",
    "        for i in range(len(index_columns_open_axis)):\n",
    "            df['index_col_' + str(i)] = df[index_columns_open_axis[i]].astype(str)\n",
    "            df.set_index(['index_col_' + str(i)], append=True, inplace=True)\n",
    "        \n",
    "        # Add to relevant list/dict\n",
    "        tables_open_axis.append(table)\n",
    "        dict_open_axis[table] = df\n",
    "    else:  # closed axis\n",
    "        tables_closed_axis.append(table)  # add to relevant list\n",
    "        # Add table to dataframe with all data from closed axis tables\n",
    "        if len(df_closed_axis) == 0:  # no data yet --> copy dataframe\n",
    "            df_closed_axis = df.copy()\n",
    "        else:  # join to existing dataframe\n",
    "            df_closed_axis = df_closed_axis.join(df)\n",
    "\n",
    "print('Closed-axis tables:')\n",
    "pprint(tables_closed_axis)\n",
    "print()\n",
    "print('Open-axis tables:')\n",
    "pprint(tables_open_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add not reported datapoints as 0's to the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform some necessary data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with all possible datapoints:\n",
    "all_datapoints = [x.replace(',,',',') for x in list(df_datapoints['tabelcode'] + ',' + df_datapoints['rij'] + ',' + df_datapoints['kolom'])]\n",
    "# List with all possible datapoints for closed axis tables:\n",
    "all_datapoints_closed = [x for x in all_datapoints if x[:13] in tables_closed_axis]\n",
    "# List with all possible datapoints for open axis tables:\n",
    "all_datapoints_open = [x for x in all_datapoints if x[:13] in tables_open_axis]\n",
    "\n",
    "# Add not reported datapoints to the dataframe with data from closed axis tables\n",
    "for column in [column for column in all_datapoints_closed if column not in list(df_closed_axis.columns)]:\n",
    "    df_closed_axis[column] = np.NaN\n",
    "df_closed_axis.fillna(0, inplace = True)\n",
    "\n",
    "# Add not reported datapoints to the dataframes with data from open axis tables\n",
    "for table in [table for table in dict_open_axis.keys()]:\n",
    "    all_datapoints_table = [x for x in all_datapoints_open if x[:13] == table]\n",
    "    for column in [column for column in all_datapoints_table if column not in list(dict_open_axis[table].columns)]:\n",
    "        dict_open_axis[table][column] = np.NaN\n",
    "    dict_open_axis[table].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare reported tables with the complete set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(np.setdiff1d(tables_complete_set, tables))\n",
    "if len(diff) == 0:\n",
    "    print('Found files for all possible tables')\n",
    "else:\n",
    "    print('No file found for the following tables:', diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DNBs Additional Validation Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNBs additional validation rules are currently published as an Excel file on the DNB statistics website. We included the Excel file here in the project.\n",
    "\n",
    "Here we read the Excel and perform some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME_RULES = '2020-01-22 Set aanvullende controleregels Solvency II_tcm46-387021.xlsx'\n",
    "\n",
    "def read_rules(entrypoint):  # function for loading the addtional rules to a dataframe, with filter on entrypoint (i.e. QRS)\n",
    "    df = pd.read_excel(join(RULES_PATH, FILENAME_RULES), header = 1)\n",
    "    df = df[(df['Standaard'] == 'SOLVENCY') | (df['Standaard'] == entrypoint)]\n",
    "    df.drop_duplicates(inplace=True) #remove double lines\n",
    "    df.fillna(\"\", inplace = True)\n",
    "    df = df.set_index('ControleRegelCode')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules = read_rules('QRS')\n",
    "df_rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate rules for tables with a closed axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluator is a piece of Python code, which takes the Additional Validation Rules as input, and transforms it to expressions that can be interpreted by the data_patterns package.\n",
    "\n",
    "The data-patterns package is also called from within the evaluator, to evaluate the results.\n",
    "\n",
    "Here we evaluate the validation rules for tables with closed axes (that are put into a single dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(df_closed_axis, df_rules, df_datapoints, PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.df_patterns.head()  # resulting patterns for closed axis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.df_results.head()  # results for closed axis tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate rules for tables with an open axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform the relevant validation rules for each table with an open axis separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if there are rules for tables with an open axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_with_rules = set([a for b in evaluator.df_rules.templates.tolist() for a in b])\n",
    "tables_with_open_axis_and_rules = tables_with_rules & set(tables_open_axis)\n",
    "print(tables_with_open_axis_and_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate rules for tables with an open axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_open_axis = {}  # dictionary with input and results per table\n",
    "for table in tables_with_open_axis_and_rules:  # loop through open axis tables for which a rule has been defined\n",
    "    info = {}\n",
    "    info['data'] = dict_open_axis[table]  # add data to dictionary\n",
    "    rule_indexes = evaluator.df_rules[evaluator.df_rules['templates'].\n",
    "                                      apply(lambda x: any([table in x]))].index  # identify rules for current table\n",
    "    info['rules'] = df_rules.loc[rule_indexes]  # add rules to dictionary\n",
    "    evaluator_open_axis = Evaluator(info['data'], info['rules'], df_datapoints, PARAMETERS)  # call evaluator\n",
    "    info['patterns'] = evaluator_open_axis.df_patterns  # add resulting patterns to dictionary\n",
    "    info['results'] = evaluator_open_axis.df_results  # add results to dictionary\n",
    "    output_open_axis[table] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tables_with_open_axis_and_rules) > 0:  # print rules if there are rules for tables with an open axis (for first table)\n",
    "    display(output_open_axis[list(tables_with_open_axis_and_rules)[0]]['rules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tables_with_open_axis_and_rules) > 0:  # print patterns if there are rules for tables with an open axis (for first table)\n",
    "    display(output_open_axis[list(tables_with_open_axis_and_rules)[0]]['patterns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tables_with_open_axis_and_rules) > 0:  # print results if there are rules for tables with an open axis (for first table)\n",
    "    display(output_open_axis[list(tables_with_open_axis_and_rules)[0]]['results'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and export results for closed and open axis tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the results from tables with closed and open axes and we can combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform results for open-axis tables, so it can be appended to results for closed-axis tables\n",
    "# The 'extra' index columns are converted to data columns\n",
    "def transform_results_open_axis(df):\n",
    "    if df.index.nlevels > 2:\n",
    "        reset_index_levels = list(range(2, df.index.nlevels))\n",
    "        df = df.reset_index(level=reset_index_levels)\n",
    "        rename_columns={}\n",
    "        for x in reset_index_levels:\n",
    "            rename_columns['level_' + str(x)] = 'id_column_' + str(x - 1)\n",
    "        df.rename(columns=rename_columns, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_patterns = evaluator.df_patterns  # patterns for closed axis tables \n",
    "df_all_results = evaluator.df_results  # results for closed axis tables\n",
    "for table in output_open_axis:  # for all open axis tables with rules -> append and sort patterns and results\n",
    "    df_all_patterns = output_open_axis[table]['patterns'].append(df_all_patterns).sort_values(by=['pattern_id']).reset_index(drop=True)\n",
    "    df_all_results = transform_results_open_axis(output_open_axis[table]['results']).append(df_all_results, sort=False).sort_values(by=['pattern_id']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe df_all_results contains all output of the evaluation of the validation rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_patterns.to_excel(join(RESULTS_PATH, \"patterns.xlsx\"))  # export patterns to excel file in results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save all results use df_all_results\n",
    "# To save all exceptions use df_all_results['result_type']==False \n",
    "# To save all confirmations use df_all_results['result_type']==True\n",
    "\n",
    "# Here we save only the exceptions to the validation rules\n",
    "df_results = df_all_results[df_all_results['result_type']==False]\n",
    "df_results.to_excel(join(RESULTS_PATH, \"results.xlsx\"))  # export results to excel file in results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_patterns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pandas code from the first pattern and evaluate it\n",
    "s = df_all_patterns.loc[0, 'pandas ex'].replace('df', 'df_closed_axis')\n",
    "print(s)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # print whole dataframe\n",
    "    display(eval(s).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
