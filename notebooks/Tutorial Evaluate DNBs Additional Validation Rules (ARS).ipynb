{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Evaluate DNBs additional Rules (ARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a tutorial for the evaluation of DNBs additional Rules for the annual Solvency II reports for solo entities.\n",
    "\n",
    "Make sure there is a file called 'ars_patterns_additional_rules' in the solvency2-rules folder.\n",
    "If not, run the 'Tutorial Convert DNBs Additional Validation Rules to Patterns' notebook first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # dataframes\n",
    "import numpy as np  # mathematical functions, arrays and matrices\n",
    "from os.path import join, isfile  # some os dependent functionality\n",
    "import data_patterns  # evaluation of patterns\n",
    "from pprint import pprint  # pretty print\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAPOINTS_PATH: path to the excel-file containing all possible datapoints (simplified taxonomy)\n",
    "# RULES_PATH: path to the excel-file with the additional rules\n",
    "# INSTANCES_DATA_PATH: path to the source data\n",
    "# RESULTS_PATH: path to the results\n",
    "DATAPOINTS_PATH = join('..', 'data', 'datapoints')\n",
    "RULES_PATH = join('..', 'solvency2-rules')\n",
    "INSTANCES_DATA_PATH = join('..', 'data', 'instances', 'all')\n",
    "RESULTS_PATH = join('..', 'results') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We log to rules.log in the data/instances path\n",
    "logging.basicConfig(filename = join(INSTANCES_DATA_PATH, 'rules.log'),level = logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file with all possible datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simplified taxonomy with all possible datapoints, located in the data/datapoints directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapoints = pd.read_csv(join(DATAPOINTS_PATH, 'ARS.csv'), sep=\";\").fillna(\"\")  # load file to dataframe\n",
    "df_datapoints.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish 2 types of tables: with a closed axis, and with an open axis.\n",
    "\n",
    "An example of a table with an open axis is the list of assets: an entity reports several 'rows of data' in the relevant table. An example of a closed axis is the balance sheet: an entity reports only 1 balance sheet per period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tables from source path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine all tables with closed axes into one DataFrame. This DataFrame is then used for all validation rules for closed axes tables. \n",
    "\n",
    "Tables with an open axis are put in a dictionary of DataFrames. We perform validation rules per table for tables with an open axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_closed_axis = []  # for listing all input tables with closed axis\n",
    "tables_open_axis = []  # for listing all input tables with open axis\n",
    "df_closed_axis = pd.DataFrame()  # one dataframe with all data from closed axis tables\n",
    "dict_open_axis = {}  # dictionary with all open axis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_complete_set = df_datapoints.tabelcode.sort_values().unique().tolist()  # list of all ARS tables\n",
    "tables = [table for table in tables_complete_set \n",
    "          if isfile(join(INSTANCES_DATA_PATH, table + '.pickle'))]  # ARS tables found in the specified INSTANCES_DATA_PATH\n",
    "tables_not_reported = [table for table in tables_complete_set if table not in tables]  # ARS tables not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    df = pd.read_pickle(join(INSTANCES_DATA_PATH, table + '.pickle'))  # read dataframe\n",
    "    \n",
    "    if df.index.nlevels > 2:  # if more than 2 indexes (entity, period), then the table has an open axis\n",
    "        # Add to relevant list/dict\n",
    "        tables_open_axis.append(table)\n",
    "        dict_open_axis[table] = df \n",
    "        \n",
    "        # Identify which columns within the open axis table make a 'table row' unique (index-columns):\n",
    "        index_columns_open_axis = [col for col in list(df.index.names) if col not in ['entity','period']]\n",
    "        \n",
    "        # Duplicate index-columns to data columns:\n",
    "        df.reset_index(level=index_columns_open_axis, inplace=True)\n",
    "        for i in range(len(index_columns_open_axis)):\n",
    "            df['index_col_' + str(i)] = df[index_columns_open_axis[i]].astype(str)\n",
    "            df.set_index(['index_col_' + str(i)], append=True, inplace=True)\n",
    "    else:  # closed axis\n",
    "        tables_closed_axis.append(table)  # add to relevant list\n",
    "        \n",
    "        # Add table to dataframe with all data from closed axis tables\n",
    "        if len(df_closed_axis) == 0:  # no data yet --> copy dataframe\n",
    "            df_closed_axis = df.copy()\n",
    "        else:  # join to existing dataframe\n",
    "            df_closed_axis = df_closed_axis.join(df)\n",
    "\n",
    "print('Closed-axis tables:')\n",
    "pprint(tables_closed_axis)\n",
    "print()\n",
    "print('Open-axis tables:')\n",
    "pprint(tables_open_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add not reported datapoints as 0's to the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform some necessary data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with all possible datapoints:\n",
    "all_datapoints = [x.replace(',,',',') for x in \n",
    "                  list(df_datapoints['tabelcode'] + ',' + df_datapoints['rij'] + ',' + df_datapoints['kolom'])]\n",
    "# List with all possible datapoints for closed axis tables:\n",
    "all_datapoints_closed = [x for x in all_datapoints if x[:13] in tables_closed_axis]\n",
    "# List with all possible datapoints for open axis tables:\n",
    "all_datapoints_open = [x for x in all_datapoints if x[:13] in tables_open_axis]\n",
    "\n",
    "# Add not reported datapoints to the dataframe with data from closed axis tables\n",
    "for col in [column for column in all_datapoints_closed if column not in list(df_closed_axis.columns)]:\n",
    "    df_closed_axis[col] = np.nan\n",
    "df_closed_axis.fillna(0, inplace = True)\n",
    "\n",
    "# Add not reported datapoints to the dataframes with data from open axis tables\n",
    "for table in [table for table in dict_open_axis.keys()]:\n",
    "    all_datapoints_table = [x for x in all_datapoints_open if x[:13] == table]\n",
    "    for col in [column for column in all_datapoints_table if column not in list(dict_open_axis[table].columns)]:\n",
    "        dict_open_axis[table][col] = np.nan\n",
    "    dict_open_axis[table].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DNBs Additional Validation Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNBs additional validation rules are currently published as an Excel file on the DNB statistics website. We included the Excel file in the project under data/downloaded files.\n",
    "\n",
    "The rules are already converted to a syntax Python can interpret, using the notebook: 'Convert DNBs Additional Validation Rules to Patterns'. In the next line of code we read these converted rules (patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patterns = pd.read_excel(join(RULES_PATH, 'ars_patterns_additional_rules.xlsx')).fillna(\"\").set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we're interested in patterns for closed-axis tables. Herefore we need to filter out:\n",
    "- patterns pointing to tables that are not reported;\n",
    "- patterns for open-axis tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patterns_closed_axis = df_patterns.copy()\n",
    "df_patterns_closed_axis = df_patterns_closed_axis[df_patterns_closed_axis['pandas ex'].apply(\n",
    "    lambda expr: not any(table in expr for table in tables_not_reported) \n",
    "    and not any(table in expr for table in tables_open_axis))]\n",
    "df_patterns_closed_axis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate patterns for tables with a closed axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have:\n",
    "- the data for closed-axis tables in a dataframe;\n",
    "- the patterns for closed-axis tables in a dataframe.\n",
    "\n",
    "To evaluate the patterns we need to create a 'PatternMiner' (part of the data_patterns package), and run the analyze function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=df_patterns_closed_axis)\n",
    "df_results_closed_axis = miner.analyze(df_closed_axis)\n",
    "df_results_closed_axis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate patterns for tables with an open axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First find the patterns defined for open-axis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patterns_open_axis = df_patterns.copy()\n",
    "df_patterns_open_axis = df_patterns_open_axis[df_patterns_open_axis['pandas ex'].apply(\n",
    "    lambda expr: any(table in expr for table in tables_open_axis))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patterns involving multiple open-axis tables are not yet supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "df_patterns_open_axis = df_patterns_open_axis[df_patterns_open_axis['pandas ex'].apply(\n",
    "    lambda expr: len(set(re.findall('S.\\d\\d.\\d\\d.\\d\\d.\\d\\d',expr)))) == 1]\n",
    "df_patterns_open_axis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we loop through the open-axis tables en evaluate the corresponding patterns on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_open_axis = {}  # dictionary with input and results per table\n",
    "for table in tables_open_axis:  # loop through open-axis tables\n",
    "    if df_patterns_open_axis['pandas ex'].apply(lambda expr: table in expr).sum() > 0:  # check if there are patterns\n",
    "        info = {}\n",
    "        info['data'] = dict_open_axis[table]  # select data\n",
    "        info['patterns'] = df_patterns_open_axis[df_patterns_open_axis['pandas ex'].apply(\n",
    "            lambda expr: table in expr)]  # select patterns\n",
    "        miner = data_patterns.PatternMiner(df_patterns=info['patterns'])\n",
    "        info['results'] = miner.analyze(info['data'])  # evaluate patterns\n",
    "        output_open_axis[table] = info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the first table (if there are rules for tables with an open axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(output_open_axis.keys()) > 0:\n",
    "    display(output_open_axis[list(output_open_axis.keys())[0]]['results'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and export results for closed and open axis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform results for open-axis tables, so it can be appended to results for closed-axis tables\n",
    "# The 'extra' index columns are converted to data columns\n",
    "def transform_results_open_axis(df):\n",
    "    if df.index.nlevels > 2:\n",
    "        reset_index_levels = list(range(2, df.index.nlevels))\n",
    "        df = df.reset_index(level=reset_index_levels)\n",
    "        rename_columns={}\n",
    "        for x in reset_index_levels:\n",
    "            rename_columns['level_' + str(x)] = 'id_column_' + str(x - 1)\n",
    "        df.rename(columns=rename_columns, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results_closed_axis.copy()  # results for closed axis tables\n",
    "for table in list(output_open_axis.keys()):  # for all open axis tables with rules -> append and sort patterns and results\n",
    "    df_results = transform_results_open_axis(output_open_axis[table]['results']).append(df_results, sort=False).sort_values(by=['pattern_id']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change column order so the dataframe starts with the identifying columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_order = []\n",
    "for i in range(1, len([col for col in list(df_results.columns) if col[:10] == 'id_column_']) + 1):\n",
    "    list_col_order.append('id_column_' + str(i))\n",
    "list_col_order.extend(col for col in list(df_results.columns) if col not in list_col_order)\n",
    "df_results = df_results[list_col_order]\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe df_all_results contains all output of the evaluation of the validation rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save all results use df_results\n",
    "# To save all exceptions use df_results['result_type']==False \n",
    "# To save all confirmations use df_results['result_type']==True\n",
    "\n",
    "# Here we save only the exceptions to the validation rules\n",
    "df_results[df_results['result_type']==False].to_excel(join(RESULTS_PATH, \"results.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pandas code from the first pattern and evaluate it\n",
    "s = df_patterns.loc[12, 'pandas ex'].replace('df', 'df_closed_axis')\n",
    "print(s)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # print whole dataframe\n",
    "    display(eval(s).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
