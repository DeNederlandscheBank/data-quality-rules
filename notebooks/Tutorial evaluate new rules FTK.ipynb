{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial describes how to evaluate new rules applicable to the assets and derivatives data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arelle import ModelManager, Cntlr, ModelFormulaObject, ModelXbrl, ViewFileFormulae, XbrlConst, ViewFileRenderedGrid\n",
    "from arelle import RenderingEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import re\n",
    "from src import Evaluator\n",
    "import logging\n",
    "import data_patterns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECIMALS = 0\n",
    "RULES_PATH = join('..', 'ftk-rules')\n",
    "INSTANCES_DATA_PATH = join('..','data','instances','...') #path of folder with converted xbrl-instance data\n",
    "TEST_DATA_PATH = join('..', 'tests', 'data', 'demo')\n",
    "RESULTS_PATH = join('..', 'results')\n",
    "DATA_PATH = join('..', 'data')\n",
    "logging.basicConfig(filename = join(RESULTS_PATH, 'rules.log'),level = logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing the new rules applicable to the assets and derivatives data. There are several sets of rules applicable to different templates:\n",
    "* K208A (Information on positions held)\n",
    "* K208B (Information on assets)\n",
    "* K208A (Information on positions held) and K208B (Information on assets)\n",
    "* K210A (Information on positions held) and K210B (Information on derivatives)\n",
    "* K210B (Information on derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_208 = pd.read_excel(join(RULES_PATH,'FTK_K208.xlsx'), engine='openpyxl')\n",
    "dfr_208B = pd.read_excel(join(RULES_PATH,'FTK_K208B.xlsx'), engine='openpyxl')\n",
    "dfr_208A = pd.read_excel(join(RULES_PATH,'FTK_K208A.xlsx'), engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_210 = pd.read_excel(join(RULES_PATH,'FTK_K210.xlsx'), engine='openpyxl')\n",
    "dfr_210B = pd.read_excel(join(RULES_PATH,'FTK_K210B.xlsx'), engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the reporting data. In the tutorial 'Convert XBRL-instances to CSV, HTML and pickles' the XBRL-instances are converted to pickle files per template. The pickle files are written to the data/instances folder. We import these pickle files. We merge dataframes for the sets of rules that are applicable to two templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and make index unique if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_208A = pd.read_pickle(join(INSTANCES_DATA_PATH,'FTK.K208A.pickle')).fillna(0).reset_index()\n",
    "# df_208A['K208A,020A'] = df_208A['K208A,020']\n",
    "# listt=list(df_208A['K208A,020A'])\n",
    "# for i in listt:\n",
    "#     lenn = len(df_208A[df_208A['K208A,020A']==i])\n",
    "#     if lenn > 1:\n",
    "#         list_ind = list(df_208A.loc[df_208A['K208A,020A']==i].index)\n",
    "#         temp = 0\n",
    "#         for j in list_ind[1:]:\n",
    "#             temp=temp+1\n",
    "#             df_208A['K208A,020A'].iloc[j] = df_208A['K208A,020A'].iloc[j] + '_' + str(temp)\n",
    "# df_208A = df_208A.set_index(['entity', 'period', 'K208A,020A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_208B = pd.read_pickle(join(INSTANCES_DATA_PATH, 'FTK.K208B.pickle')).fillna(0).reset_index()\n",
    "# df_208B = df_208B.set_index(['entity', 'period', 'K208B,130'])\n",
    "# df_208B['K208B,130'] = df_208B.index.get_level_values(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_208 = pd.merge(pd.read_pickle(join(INSTANCES_DATA_PATH,'FTK.K208A.pickle')).reset_index(),pd.read_pickle(join(INSTANCES_DATA_PATH,'FTK.K208B.pickle')).reset_index(),how='inner', left_on=['entity','period','K208A,020'], right_on=['entity','period','K208B,130']).set_index(['entity', 'period', 'K208A,020'])\n",
    "# df_208 = df_208.fillna(0).reset_index()\n",
    "# df_208['K208A,020A'] = df_208['K208A,020']\n",
    "# listt=list(df_208['K208A,020A'])\n",
    "# for i in listt:\n",
    "#     lenn = len(df_208[df_208['K208A,020A']==i])\n",
    "#     if lenn > 1:\n",
    "#         list_ind = list(df_208.loc[df_208['K208A,020A']==i].index)\n",
    "#         temp = 0\n",
    "#         for j in list_ind[1:]:\n",
    "#             temp=temp+1\n",
    "#             df_208['K208A,020A'].iloc[j] = df_208['K208A,020A'].iloc[j] + '_' + str(temp)\n",
    "# df_208 = df_208.set_index(['entity', 'period', 'K208A,020A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we work with dummy data in order to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_208A = pd.read_pickle(join(TEST_DATA_PATH,'FTK.K208A.pickle')).reset_index() #Import demo pickles\n",
    "df_208B = pd.read_pickle(join(TEST_DATA_PATH,'FTK.K208B.pickle')).reset_index() #Import demo pickles\n",
    "df_208 = pd.merge(df_208A,df_208B,how='inner', left_on=['entity', 'period', 'K208A,020'], right_on=['entity','period', 'K208B,130']).set_index(['entity', 'period', 'K208A,020'])\n",
    "df_208B = df_208B.set_index(['entity','period', 'K208B,130'])\n",
    "df_208A = df_208A.set_index(['entity', 'period', 'K208A,020'])\n",
    "df_208B['K208B,130'] = df_208B.index.get_level_values(2)\n",
    "df_208['K208A,020'] = df_208.index.get_level_values(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_210B = pd.read_pickle(join(INSTANCES_DATA_PATH, 'FTK.K210B.pickle')).fillna(0).reset_index()\n",
    "# df_210B = df_210B.set_index(['entity', 'period', 'K210B,200'])\n",
    "# df_210B['K210B,200'] = df_210B.index.get_level_values(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_210 = pd.merge(pd.read_pickle(join(INSTANCES_DATA_PATH,'FTK.K210A.pickle')).reset_index(),pd.read_pickle(join(INSTANCES_DATA_PATH,'FTK.K210B.pickle')).reset_index(),how='inner', left_on=['entity','period','K210A,020'], right_on=['entity','period','K210B,200']).set_index(['entity', 'period', 'K210A,020'])\n",
    "# df_210 = df_210.fillna(0).reset_index()\n",
    "# df_210['K210A,020A'] = df_210['K210A,020']\n",
    "# listt=list(df_210['K210A,020A'])\n",
    "# for i in listt:\n",
    "#     lenn = len(df_210[df_210['K210A,020A']==i])\n",
    "#     if lenn > 1:\n",
    "#         list_ind = list(df_210.loc[df_210['K210A,020A']==i].index)\n",
    "#         temp = 0\n",
    "#         for j in list_ind[1:]:\n",
    "#             temp=temp+1\n",
    "#             df_210['K210A,020A'].iloc[j] = df_210['K210A,020A'].iloc[j] + '_' + str(temp)\n",
    "# df_210 = df_210.set_index(['entity', 'period', 'K210A,020A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we work with dummy data in order to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_210A = pd.read_pickle(join(TEST_DATA_PATH,'FTK.K210A.pickle')).reset_index() #Import demo pickles\n",
    "df_210B = pd.read_pickle(join(TEST_DATA_PATH, 'FTK.K210B.pickle')).reset_index() #Import demo pickles\n",
    "df_210 = pd.merge(df_210A,df_210B,how='inner', left_on=['entity', 'period', 'K210A,020'], right_on=['entity', 'period', 'K210B,200']).set_index(['entity', 'period', 'K210A,020'])\n",
    "df_210A = df_210A.set_index(['entity', 'period', 'K210A,020'])\n",
    "df_210B = df_210B.set_index(['entity', 'period', 'K210B,200'])\n",
    "df_210B['K210B,200'] = df_210B.index.get_level_values(2)\n",
    "df_210['K210A,020'] = df_210.index.get_level_values(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate the different sets of rules. First, we construct a PatternMiner-object with the data-patterns package using the rules dataframe. Second, we use the analyze-function to get the results of the rules. We do this for each set of rules separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_208)\n",
    "results_208 = miner.analyze(df_208)\n",
    "results_208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_208B)\n",
    "results_208B = miner.analyze(df_208B)\n",
    "results_208B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_208A)\n",
    "results_208A = miner.analyze(df_208A)\n",
    "results_208A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_210)\n",
    "results_210 = miner.analyze(df_210)\n",
    "results_210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner2 = data_patterns.PatternMiner(df_patterns=dfr_210B)\n",
    "results2_210B = miner2.analyze(df_210B)\n",
    "results2_210B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
