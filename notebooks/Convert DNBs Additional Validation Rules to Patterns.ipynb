{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert DNBs Additional Validation Rules to Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNBs additional validation rules are available in the 'solvency2-rules' subfolder of the repository.  \n",
    "The formulas in this file use a specific syntax, this notebook converts this syntax to a syntax that can be interpreted by Python.  \n",
    "The resulting formulas are called 'patterns'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # dataframes\n",
    "from os.path import join # some os dependent functionality\n",
    "# from dqr import Evaluator  # conversion from 'rules' to expressions for the data-patterns packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location and name of the file with the additional rules:\n",
    "RULES_PATH = join('..', 'data', 'downloaded files')  \n",
    "FILENAME_RULES = '2021_04-01_set_aanvullende_controleregels_solvency2.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location and names of files with all possible datapoints for QRS and ARS\n",
    "DATAPOINTS_PATH = join('..', 'data', 'datapoints')\n",
    "FILENAME_DATAPOINTS_QRS = 'QRS.csv'\n",
    "FILENAME_DATAPOINTS_ARS = 'ARS.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters:\n",
    "PARAMETERS = {'decimal': 0}\n",
    "# currently only 'decimal' is available which specifies tolerance during evaluation of patterns.\n",
    "# decimal: 0 means tolerance = abs(1.5e-0) (= 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We log to rules.log in the data/instances path\n",
    "# logging.basicConfig(filename = join(INSTANCES_DATA_PATH, 'rules.log'),level = logging.INFO, \n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file with all possible datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simplified taxonomy with all possible datapoints, located in the data/datapoints directory.  \n",
    "The evaluator uses this taxonomy to generate the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files to dataframe:\n",
    "df_datapoints_qrs = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_QRS), sep=\";\").fillna(\"\")\n",
    "df_datapoints_ars = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_ARS), sep=\";\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapoints_ars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DNBs Additional Validation Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNBs additional validation rules are currently published as an Excel file on the DNB statistics website. We included the Excel file here in the project.\n",
    "\n",
    "Here we read the Excel and perform some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules = pd.read_excel(join(RULES_PATH, FILENAME_RULES), header = 1, engine='openpyxl')\n",
    "df_rules.drop_duplicates(inplace=True) #remove double lines\n",
    "df_rules.fillna(\"\", inplace = True)\n",
    "df_rules = df_rules.set_index('ControleRegelType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<> \" \" has to be converted to <> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules['Formule'] = df_rules['Formule'].str.replace('\" \"','None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rules check dates to be filled by > 0, this has to be changed to <> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rules = ['S.15.01_105',\n",
    "                 'S.15.01_107',\n",
    "                 'S.23.04_111',\n",
    "                 'S.23.04_112',\n",
    "                 'S.23.04_121',\n",
    "                 'S.23.04_122',\n",
    "                 'S.23.04_133',\n",
    "                 'S.23.04_144',\n",
    "                 'S.23.04_145', \n",
    "                 'S.30.01_105',\n",
    "                 'S.30.01_106',\n",
    "                 'S.30.01_117',\n",
    "                 'S.30.01_118',\n",
    "                 'S.30.03_102',\n",
    "                 'S.30.03_103',\n",
    "                 'S.36.01_106',\n",
    "                 'S.36.02_106',\n",
    "                 'S.36.02_108',\n",
    "                 'S.36.03_104',\n",
    "                 'S.10.01_115',\n",
    "                 'S.15.01_106',\n",
    "                 'S.15.01_108',\n",
    "                 'S.23.04_127',\n",
    "                 'S.23.04_128',\n",
    "                 'S.23.04_137',\n",
    "                 'S.23.04_148',\n",
    "                 'S.23.04_149']\n",
    "\n",
    "df_rules.loc[list_of_rules, 'Formule'] = df_rules.loc[list_of_rules, 'Formule'].str.replace(\"> 0\",'<> None').str.replace(\">0\",'<> None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Excel file contains rules for different report-types. In the next step we filter out the rules for QRS and ARS respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules_qrs = df_rules.copy()[(df_rules['Standaard'] == 'SOLVENCY') | (df_rules['Standaard'] == 'QRS')]\n",
    "df_rules_ars = df_rules.copy()[(df_rules['Standaard'] == 'SOLVENCY') | (df_rules['Standaard'] == 'ARS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules_qrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules_ars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the rules to patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluator is a piece of Python code, which takes the Additional Validation Rules as input, and transforms it to expressions that can be interpreted by the data_patterns package (patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import sys\n",
    "import data_patterns\n",
    "import logging\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, df_rules, df_datapoints, parameters):\n",
    "\n",
    "        self.entrypoint_templates = sorted(list(df_datapoints['tabelcode'].unique()))\n",
    "        self.entrypoint_datapoints = sorted(list((df_datapoints['tabelcode'] + \",\" +\n",
    "                                                  df_datapoints['rij'] + \",\" +\n",
    "                                                  df_datapoints['kolom']\n",
    "                                                  ).str.replace(\",,\", \",\")))\n",
    "        self.df_rules = self.pre_process_rules(df_rules)\n",
    "        self.df_patterns = self.process_rules(df_datapoints, parameters)\n",
    "\n",
    "    def datapoints2pandas(self, s):\n",
    "        \"\"\"Transform EVA2 datapoints to Python Pandas datapoints by making letters uppercase\"\"\"\n",
    "        datapoints = []\n",
    "        for item in list(set(re.findall(r'{(.*?)}', s))):\n",
    "            datapoints.append(item.upper())\n",
    "            s = s.replace(item, '\"' + item.upper() + '\"')\n",
    "        s = self.preprocess_pattern(s)\n",
    "        return s, datapoints\n",
    "\n",
    "    def replace_and_or(self, s):\n",
    "        \"\"\"Replace and by & and or by |, but not within strings\"\"\"\n",
    "        if re.search(r\"(.*?)\\\"(.*?)\\\"(.*)\", s) is None:  # input text does not contain strings\n",
    "            s = s.replace(\"OR\", \"|\")\n",
    "            s = s.replace(\"AND\", \"&\")\n",
    "        for item in re.findall(r\"(.*?)\\\"(.*?)\\\"(.*)\", s):\n",
    "            s = s.replace(item[0], item[0].replace(\"OR\", \"|\"))\n",
    "            s = s.replace(item[0], item[0].replace(\"AND\", \"&\"))\n",
    "            s = s.replace(item[2], self.replace_and_or(item[2]))\n",
    "        return s\n",
    "\n",
    "    def preprocess_pattern(self, pattern):\n",
    "\n",
    "        pattern = pattern.replace(\"<>\", \"!=\")\n",
    "        pattern = pattern.replace(\"< >\", \"!=\")  # the space between < and > should be deleted in EVA2\n",
    "        pattern = pattern.replace(';', \",\")  # this should be corrected in EVA2\n",
    "\n",
    "        # pattern = pattern.replace('\"', \"'\")\n",
    "        pattern = self.replace_and_or(pattern)\n",
    "        return pattern\n",
    "\n",
    "    def make_pattern_expression(self, expression, name, parameters):\n",
    "        \"\"\"Make expressions for the miner\"\"\"\n",
    "        parameters['solvency'] = True\n",
    "        pandas_expressions = data_patterns.to_pandas_expressions(expression, {}, parameters, None)\n",
    "        pattern = [[name, 0] + [expression] + [0, 0, 0] + [\"DNB\"] + [{}] + pandas_expressions + [\"\", \"\", \"\"]]\n",
    "        return pattern\n",
    "\n",
    "    def pre_process_rules(self, df_rules):\n",
    "        \"\"\"Transform rules so that we can evaluate them. Not all rules are fit to be evaluated\"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        df_rules['datapoints'] = ''\n",
    "        df_rules['datapoints'] = df_rules['datapoints'].astype('object')\n",
    "        df_rules['templates'] = ''\n",
    "        df_rules['templates'] = df_rules['templates'].astype('object')\n",
    "        for row in df_rules.index:\n",
    "            rule_original = df_rules.loc[row, 'Formule']\n",
    "            if not isinstance(rule_original, str):\n",
    "                logger.info(\"Rule \" + row + \": \" + \"duplicate rule. \")\n",
    "                rule_original = rule_original.values[0]\n",
    "            rule_original, datapoints = self.datapoints2pandas(rule_original)\n",
    "            df_rules.at[row, 'datapoints'] = datapoints\n",
    "            df_rules.at[row, 'templates'] = list(set([datapoint[0:13].upper() for datapoint in datapoints]))\n",
    "            df_rules.loc[row, 'Formule_input'] = rule_original\n",
    "\n",
    "        df_rules['error'] = ''  # error message\n",
    "        df_rules['n_patterns'] = 0  # number of patterns derived from rules\n",
    "\n",
    "        return df_rules\n",
    "\n",
    "    def process_rule(self, pre_expression, name, datapoints, expansion_dict, df_datapoints, parameters):\n",
    "        \"\"\"Some rules have multiple rows or columns. This function makes all the expressions with every row/column\"\"\"\n",
    "        expressions = []\n",
    "        invalid_expressions = \"\"\n",
    "        if expansion_dict:\n",
    "            if datapoints[0] in expansion_dict.keys():\n",
    "                zero = datapoints[0]\n",
    "            else:\n",
    "                zero = datapoints[1]\n",
    "            for i in range(len(expansion_dict[zero])):\n",
    "                expression = pre_expression\n",
    "                valid_expression = True\n",
    "                for datapoint in datapoints:\n",
    "                    if datapoint in expansion_dict.keys():\n",
    "                        if len(df_datapoints[(df_datapoints['tabelcode'] == expansion_dict[datapoint][i][:13]) &\n",
    "                                             (df_datapoints['rij'] == expansion_dict[datapoint][i][14:19].upper()) &\n",
    "                                             (df_datapoints['kolom'] == expansion_dict[datapoint][i][20:26].upper())]) == 0:\n",
    "                            valid_expression = False\n",
    "                        expression = expression.replace(datapoint, expansion_dict[datapoint][i])\n",
    "                if valid_expression:\n",
    "                    expressions.extend(self.make_pattern_expression(expression, name, parameters))\n",
    "                else:\n",
    "                    if invalid_expressions == \"\":\n",
    "                        invalid_expressions = invalid_expressions + \"(\" + expression + \")\"\n",
    "                    else:\n",
    "                        invalid_expressions = invalid_expressions + \", (\" + expression + \")\"\n",
    "        else:\n",
    "            expressions.extend(self.make_pattern_expression(pre_expression, name, parameters))\n",
    "\n",
    "        return expressions, invalid_expressions\n",
    "\n",
    "    def process_rules(self, df_datapoints, parameters):\n",
    "        \"\"\"Evaluate all rules and stores the result in df_rules\"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        rules_expressions = []\n",
    "        for idx in range(len(self.df_rules.index)):\n",
    "            row = self.df_rules.index[idx]\n",
    "            rule_original = self.df_rules.loc[row, 'Formule_input']\n",
    "            if (\"SUBSTR\" not in rule_original) and \\\n",
    "               (\"NOT CONTAINS\" not in rule_original) and \\\n",
    "               (\"CONTAINS\" not in rule_original) and \\\n",
    "               (\"SUM\" not in rule_original) and \\\n",
    "               (\"= .\" not in rule_original) and \\\n",
    "               (\"=.\" not in rule_original):\n",
    "                rule_name = self.df_rules.index[idx]\n",
    "                datapoints = self.df_rules.loc[row, 'datapoints'].copy()\n",
    "                templates = self.df_rules.loc[row, 'templates']\n",
    "                self.df_rules['Rijen'] = self.df_rules['Rijen'].astype(str)\n",
    "                self.df_rules['Kolommen'] = self.df_rules['Kolommen'].astype(str)\n",
    "                row_range = self.df_rules.loc[row, 'Rijen'].replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \";\").split(\";\")\n",
    "                column_range = self.df_rules.loc[row, 'Kolommen'].replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \";\").split(\";\")\n",
    "                # are the templates in the rule in the instance?\n",
    "                templates_not_found = []\n",
    "                for template in templates:\n",
    "                    if template not in self.entrypoint_templates:\n",
    "                        templates_not_found.append(template)\n",
    "\n",
    "                if templates_not_found == []:\n",
    "                    datapoints_not_found = []\n",
    "                    expansion_dict = {}\n",
    "                    # are the datapoints in the rule in the instance?\n",
    "                    for datapoint in datapoints:\n",
    "                        if datapoint not in self.entrypoint_datapoints:  # if datapoint is not there, see if we need to add rows or columns\n",
    "                            new_list = []\n",
    "                            if datapoint[14] == \"C\" and (len(row_range) > 1 or row_range[0].upper() == \"ALL\"):\n",
    "                                if len(row_range) == 1 and row_range[0].upper() == \"ALL\":\n",
    "                                    for col in self.entrypoint_datapoints:\n",
    "                                        reg = re.search(datapoint[0:14] + \"R....,\" + datapoint[14:],col)  # do for all rows if necessary\n",
    "                                        if reg:\n",
    "                                            new_list.append(reg.group(0))\n",
    "                                else:\n",
    "                                    rows = []\n",
    "                                    for r in row_range:\n",
    "                                        if len(r) - len(r.replace(\"-\", \"\")) == 1:  # range\n",
    "                                            low, high = r.split(\"-\")\n",
    "                                            rows.extend(list(df_datapoints[(df_datapoints['tabelcode'] == datapoint[0:13]) &\n",
    "                                                                           (df_datapoints['kolom'] == datapoint[14:]) &\n",
    "                                                                           (df_datapoints['rij'].str[-4:] >= low) &\n",
    "                                                                           (df_datapoints['rij'].str[-4:] <= high)\n",
    "                                                                           ].rij))\n",
    "                                        else:\n",
    "                                            if r.upper()[0] == 'R':\n",
    "                                                rows.extend([r.upper()])\n",
    "                                            else:\n",
    "                                                rows.extend([('R' + r)])\n",
    "                                    for r in rows:\n",
    "                                        new_list.append(datapoint[0:14] + r + \",\" + datapoint[14:])\n",
    "                            if datapoint[14] == \"R\" and(len(column_range) > 1 or column_range[0].upper() == \"ALL\"):\n",
    "                                if len(column_range) == 1 and column_range[0].upper() == \"ALL\":\n",
    "                                    for col in self.entrypoint_datapoints:\n",
    "                                        reg = re.search(datapoint + \",C....\", col)  # do for all columns if necessary\n",
    "                                        if reg:\n",
    "                                            new_list.append(reg.group(0))\n",
    "                                else:\n",
    "                                    cols = []\n",
    "                                    for c in column_range:\n",
    "                                        if len(c) - len(c.replace(\"-\", \"\")) == 1:  # range\n",
    "                                            low, high = c.split(\"-\")\n",
    "                                            cols.extend(list(df_datapoints[(df_datapoints['tabelcode'] == datapoint[0:13]) &\n",
    "                                                                           (df_datapoints['rij'] == datapoint[14:]) &\n",
    "                                                                           (df_datapoints['kolom'].str[-4:] >= low) &\n",
    "                                                                           (df_datapoints['kolom'].str[-4:] <= high)\n",
    "                                                                           ].rij))\n",
    "                                        else:\n",
    "                                            if c.upper()[0] == 'C':\n",
    "                                                cols.extend([c.upper()])\n",
    "                                            else:\n",
    "                                                cols.extend([('C' + c)])\n",
    "                                    for c in cols:\n",
    "                                        new_list.append(datapoint + \",\" + c)\n",
    "                            if new_list != []:\n",
    "                                expansion_dict[datapoint] = new_list\n",
    "                            else:\n",
    "                                datapoints_not_found.append(datapoint)\n",
    "                    if datapoints_not_found == []:\n",
    "                        rule_expressions, invalid_expressions = self.process_rule(rule_original, rule_name, datapoints, expansion_dict, df_datapoints, parameters)\n",
    "                        rules_expressions.extend(rule_expressions)\n",
    "                        if invalid_expressions != \"\":\n",
    "                            self.df_rules.loc[row, 'error'] = \\\n",
    "                                'Some expressions skipped due to invalid datapoint references: ' + invalid_expressions\n",
    "                            logger.warning(\"Rule \" + row + \", \" + self.df_rules.loc[row, 'error'])\n",
    "                        else:\n",
    "                            self.df_rules.loc[row, 'error'] = ''\n",
    "                        self.df_rules.loc[row, 'n_patterns'] = len(rule_expressions)\n",
    "                        logger.info(\"Rule \" + row + \", \" + str(len(rule_expressions)) + \" pattern(s) generated\")\n",
    "                    else:\n",
    "                        # expression = rule_original\n",
    "                        self.df_rules.loc[row, 'error'] = 'missing datapoint(s): ' + str(datapoints_not_found)\n",
    "                        logger.warning(\"Rule \" + row + \", \" + self.df_rules.loc[row, 'error'])\n",
    "                else:\n",
    "                    # expression = rule_original\n",
    "                    self.df_rules.loc[row, 'error'] = 'missing template(s): ' + str(templates_not_found)\n",
    "                    logger.warning(\"Rule \" + row + \", \" + self.df_rules.loc[row, 'error'])\n",
    "\n",
    "        df_patterns = pd.DataFrame(data = rules_expressions, columns = data_patterns.PATTERNS_COLUMNS)\n",
    "        df_patterns.index.name = 'index'\n",
    "\n",
    "        return df_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_qrs = Evaluator(df_rules_qrs, df_datapoints_qrs, PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_qrs.df_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules_ars_1 = df_rules_ars.iloc[0:400, :]\n",
    "\n",
    "evaluator_ars = Evaluator(df_rules_ars, df_datapoints_ars, PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_ars.df_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator_qrs.df_patterns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_ars.df_patterns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export patterns to rules folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_qrs.df_patterns.to_excel(join('..', 'solvency2-rules', \"qrs_patterns_additional_rules.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_ars.df_patterns.to_excel(join('..', 'solvency2-rules', \"ars_patterns_additional_rules.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
