{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert DNBs Additional Validation Rules to Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNBs additional validation rules are available in the 'solvency2-rules' subfolder of the repository.  \n",
    "The formulas in this file use a specific syntax, this notebook converts this syntax to a syntax that can be interpreted by Python.  \n",
    "The resulting formulas are called 'patterns'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # dataframes\n",
    "from os.path import join # some os dependent functionality\n",
    "import re # regex\n",
    "#from src import adjust_syntax  # adjust syntax of additional Solvency 2 validation rules\n",
    "#from src import Evaluator  # conversion from 'rules' to pandas expressions for the data-patterns packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location and name of the file with the additional rules:\n",
    "RULES_PATH = join('..', 'data', 'downloaded files')  \n",
    "# Based on 2022\n",
    "FILENAME_RULES = '2022_02_23_set_aanvullende_controleregels_solvency2.xlsx'\n",
    "# Based on 2021\n",
    "#FILENAME_RULES = '2021_04-01_set_aanvullende_controleregels_solvency2.xlsx'\n",
    "# Based on 2020\n",
    "#FILENAME_RULES = '2020-01-22 Set aanvullende controleregels Solvency II_tcm46-387021.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location and names of files with all possible datapoints for QRS and ARS\n",
    "DATAPOINTS_PATH = join('..', 'data', 'datapoints')\n",
    "FILENAME_DATAPOINTS_QRS = 'QRS.csv'\n",
    "FILENAME_DATAPOINTS_ARS = 'ARS.csv'\n",
    "FILENAME_DATAPOINTS_QRG = 'QRG.csv'\n",
    "FILENAME_DATAPOINTS_ARG = 'ARG.csv'\n",
    "FILENAME_DATAPOINTS_QFS = 'QFS.csv'\n",
    "FILENAME_DATAPOINTS_QFG = 'QFG.csv'\n",
    "#FILENAME_DATAPOINTS_FTK = 'FTK.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters:\n",
    "PARAMETERS = {'decimal': 0}\n",
    "# currently only 'decimal' is available which specifies tolerance during evaluation of patterns.\n",
    "# decimal: 0 means tolerance = abs(1.5e-0) (= 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We log to rules.log in the data/instances path\n",
    "# logging.basicConfig(filename = join(INSTANCES_DATA_PATH, 'rules.log'),level = logging.INFO, \n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file with all possible datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simplified taxonomy with all possible datapoints, located in the data/datapoints directory.  \n",
    "The evaluator uses this taxonomy to generate the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files to dataframe:\n",
    "df_datapoints_qrs = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_QRS), sep=\";\").fillna(\"\")\n",
    "df_datapoints_ars = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_ARS), sep=\";\").fillna(\"\")\n",
    "df_datapoints_qrg = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_QRG), sep=\";\").fillna(\"\")\n",
    "df_datapoints_arg = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_ARG), sep=\";\").fillna(\"\")\n",
    "df_datapoints_qfs = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_QFS), sep=\";\").fillna(\"\")\n",
    "df_datapoints_qfg = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_QFG), sep=\";\").fillna(\"\")\n",
    "#df_datapoints_ftk = pd.read_csv(join(DATAPOINTS_PATH, FILENAME_DATAPOINTS_FTK), sep=\";\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into a single dataframe with all datapoints\n",
    "df_datapoints = pd.concat([df_datapoints_qrs, df_datapoints_ars, df_datapoints_qrg, df_datapoints_arg,\n",
    "                           df_datapoints_qfs, df_datapoints_qfg, #df_datapoints_ftk\n",
    "                          ], ignore_index = True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapoints.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DNBs Additional Validation Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNBs additional validation rules are currently published as an Excel file on the DNB statistics website. We included the Excel file here in the project.\n",
    "\n",
    "Here we read the Excel and perform some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules = pd.read_excel(join(RULES_PATH, FILENAME_RULES), header = 1, engine='openpyxl')\n",
    "df_rules.drop_duplicates(inplace=True) #remove double lines\n",
    "df_rules.fillna(\"\", inplace = True)\n",
    "df_rules = df_rules.set_index('ControleRegelCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust syntax of additional Solvency 2 validation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_syntax(rules):\n",
    "    \"\"\"Adjust syntax of additional Solvency 2 validation rules\"\"\"\n",
    "\n",
    "    # Correct template typo's\n",
    "    rules['Formule'] = rules['Formule'].str.replace('S.08.01.01.01,c0380', 'S.08.01.01.02,c0380')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('S.08.01.01.01,c0430', 'S.08.01.01.02,c0430')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('S.08.01.04.01,c0380', 'S.08.01.04.02,c0380')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('S.08.01.04.01,c0430', 'S.08.01.04.02,c0430')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('S.08.02.01.01,c0320', 'S.08.02.01.02,c0320')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('S.08.02.04.01,c0320', 'S.08.02.04.02,c0320')\n",
    "\n",
    "    # \" \" has to be converted to None\n",
    "    rules['Formule'] = rules['Formule'].str.replace('\" \"','None')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('\"\"','None')\n",
    "    # <> . has to be converted to <> None\n",
    "    rules['Formule'] = rules['Formule'].str.replace('<> \\.','<> None')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('<>\\.','<> None')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('< > \\.','< > None')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('< >\\.','< > None')\n",
    "    # = . has to be converted to = None\n",
    "    rules['Formule'] = rules['Formule'].str.replace('= \\.','= None')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('=\\.','= None')\n",
    "    # <> has to be converted to !=\n",
    "    rules['Formule'] = rules['Formule'].str.replace('<>','!=')\n",
    "    rules['Formule'] = rules['Formule'].str.replace('< >','!=')\n",
    "    # ; has to be converted to ,\n",
    "    rules['Formule'] = rules['Formule'].str.replace(';',',')\n",
    "\n",
    "    # Use of rNNN is unnecessary\n",
    "    rules['Formule'] = rules['Formule'].str.replace('rNNN,','')\n",
    "    # Use wildcard # instead of RNNN for summing instead of repeating over multiple rows, and make sure all rows are included\n",
    "    rules.loc[rules['Formule'].str.contains('RNNN,'), 'Rijen'] = '(All)'\n",
    "    rules['Formule'] = rules['Formule'].str.replace('RNNN,','#,')\n",
    "    # Correct C\\d\\d\\d\\dC to C\\d\\d\\d\\d\n",
    "    for item in [tuple(filter(None, tup)) for cols in rules['Kolommen'] for tup in re.findall(r\"([Cc]\\d\\d\\d\\d)([Cc])\", cols)]:\n",
    "        rules['Kolommen'] = rules['Kolommen'].str.replace(\"\".join(item),item[0])\n",
    "    # Correct C\\d\\d\\d to C0\\d\\d\\d\n",
    "    for item in [tuple(filter(None, tup)) for cols in rules['Kolommen'] for tup in re.findall(r\"([^0-9])(\\d\\d\\d)($|;|\\))\", cols)]:\n",
    "        item = tuple((item[0], item[1], \"\")) if len(item) == 2 else tuple((item[0], item[1], item[2]))\n",
    "        rules['Kolommen'] = rules['Kolommen'].str.replace(\"\".join(item),item[0] + '0' + item[1] + item[2])\n",
    "    # Split double row entries {R\\d\\d\\d\\d,R\\d\\d\\d\\d} into two entries, i.e. {R\\d\\d\\d\\d},{R\\d\\d\\d\\d}\n",
    "    for item in [tuple(filter(None, tup)) for form in rules['Formule'] for tup in re.findall(r\"([Rr]\\d\\d\\d\\d)(,)([Rr]\\d\\d\\d\\d)\", form)]:\n",
    "        rules['Formule'] = rules['Formule'].str.replace(\"\".join(item),item[0] + \"}\" + item[1] + \"{\" + item[2])\n",
    "    # Add template when not included in formula\n",
    "    for item in [tuple((rules.loc[idx, 'HoofdTabel'],tuple(filter(None, tup)))) for idx in list(rules.index) for tup in re.findall(r\"(\\{)([CcRr]\\d\\d\\d\\d\\})\", rules.loc[idx, 'Formule'])]:\n",
    "        rules['Formule'] = rules['Formule'].str.replace(\"\".join(item[1]),item[1][0] + item[0] + \",\" + item[1][1])\n",
    "    # Add comma to SUBSTR({}#,#) expression\n",
    "    for item in [tuple(filter(None, tup)) for form in rules['Formule'] for tup in re.findall(r\"(\\})([0-9]{1,2})\", form)]:\n",
    "        rules['Formule'] = rules['Formule'].str.replace(\"\".join(item), item[0] + \",\" + item[1])\n",
    "    # Remove trailing comma in (#,#,#,) expression\n",
    "    rules['Formule'] = rules['Formule'].str.replace(r\",\\)\",\")\")\n",
    "\n",
    "    # Some rules check dates to be filled by > 0, this has to be changed to <> None\n",
    "    list_of_rules = ['S.15.01_105', 'S.15.01_107', 'S.23.04_111', 'S.23.04_112', 'S.23.04_121', 'S.23.04_122', 'S.23.04_133', 'S.23.04_144', 'S.23.04_145', 'S.30.01_105', 'S.30.01_106', \n",
    "                     'S.30.01_117', 'S.30.01_118', 'S.30.03_102', 'S.30.03_103', 'S.36.01_106', 'S.36.02_106', 'S.36.02_108', 'S.36.03_104', 'S.10.01_115', 'S.15.01_106', 'S.15.01_108',\n",
    "                     'S.23.04_127', 'S.23.04_128', 'S.23.04_137', 'S.23.04_148', 'S.23.04_149']\n",
    "    list_of_rules_adj = [rule for rule in list_of_rules if rule in list(rules.index)]\n",
    "    if len(list_of_rules_adj) > 0:\n",
    "        rules.loc[list_of_rules_adj, 'Formule'] = rules.loc[list_of_rules_adj, 'Formule'].str.replace(\"> 0\",'<> None').str.replace(\">0\",'<> None')\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules = adjust_syntax(df_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Excel file contains rules for different report-types. In the next step we filter out the rules for QRS, ARS, QRG and ARG respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules_qrs = df_rules.copy()[(df_rules['Standaard'] == 'SOLVENCY') | (df_rules['Standaard'] == 'QRS')]\n",
    "df_rules_ars = df_rules.copy()[(df_rules['Standaard'] == 'SOLVENCY') | (df_rules['Standaard'] == 'ARS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules_qrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules_ars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the rules to patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluator is a piece of Python code, which takes the Additional Validation Rules as input, and transforms it to expressions that can be interpreted by the data_patterns package (patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import sys\n",
    "import data_patterns\n",
    "import logging\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, df_rules, df_datapoints, parameters):\n",
    "\n",
    "        self.entrypoint_templates = sorted(list(df_datapoints['tabelcode'].unique()))\n",
    "        self.entrypoint_datapoints = sorted(list((df_datapoints['tabelcode'] + \",\" +\n",
    "                                                  df_datapoints['rij'] + \",\" +\n",
    "                                                  df_datapoints['kolom']\n",
    "                                                  ).str.replace(\",,\", \",\")))\n",
    "        self.df_rules = self.pre_process_rules(df_rules)\n",
    "        self.df_patterns = self.process_rules(df_datapoints, parameters)\n",
    "\n",
    "    def datapoints2pandas(self, s):\n",
    "        \"\"\"Transform EVA2 datapoints to Python Pandas datapoints by making letters uppercase\"\"\"\n",
    "        datapoints = []\n",
    "        for item in list(set(re.findall(r'{(.*?)}', s))):\n",
    "            datapoints.append(item.upper())\n",
    "            s = s.replace(item, '\"' + item.upper() + '\"')\n",
    "        s = self.preprocess_pattern(s)\n",
    "        return s, datapoints\n",
    "\n",
    "    def replace_and_or(self, s):\n",
    "        \"\"\"Replace and by & and or by |, but not within strings\"\"\"\n",
    "        if re.search(r\"(.*?)\\\"(.*?)\\\"(.*)\", s) is None:  # input text does not contain strings\n",
    "            s = s.replace(\"OR\", \"|\")\n",
    "            s = s.replace(\"AND\", \"&\")\n",
    "        for item in re.findall(r\"(.*?)\\\"(.*?)\\\"(.*)\", s):\n",
    "            s = s.replace(item[0], item[0].replace(\"OR\", \"|\"))\n",
    "            s = s.replace(item[0], item[0].replace(\"AND\", \"&\"))\n",
    "            s = s.replace(item[2], self.replace_and_or(item[2]))\n",
    "        return s\n",
    "\n",
    "    def replace_substr(self, s):\n",
    "        \"\"\"Replace SUBSTR(A,B,C) by A.str.slice(B,B+C,1)\"\"\"\n",
    "        for item in re.findall(r\"(SUBSTR\\s?\\()(.*?)(,)([0-9]{1,2})(,)([0-9]{1,2})(\\))\", s):\n",
    "            s = s.replace(\"\".join(item), item[1] + \".str.slice(\" + str(int(item[3]) - int(1)) + \",\" + str(int(item[3]) - int(1) + int(item[5])) + \",1)\")\n",
    "        return s\n",
    "\n",
    "    def replace_in_not_in(self, s):\n",
    "        \"\"\"Replace IN and NOT IN by str.contains((...))\"\"\"\n",
    "        # NOT IN\n",
    "        for item in re.findall(r\"(.*?)(\\s?[^\\w]NOT IN[^\\w]\\s?)(\\(.*?\\))\", s):\n",
    "            item_2_adj = item[2] if \"None\" not in item[2] else \"(\" + item[2].replace(\"None, \", \"\").replace(\"None,\", \"\").replace(\", None\", \"\").replace(\",None\", \"\") + \", True, 0, True)\"\n",
    "            s = s.replace(\"\".join(item),item[0] + \".str.contains\" + item[2].replace('\",\"', \"|\").replace('\", \"', \"|\") + \"=False\")\n",
    "        # IN\n",
    "        for item in re.findall(r\"(.*?)(\\s?[^\\w]IN[^\\w]\\s?)(\\(.*?\\))\", s):\n",
    "            item_2_adj = item[2] if \"None\" not in item[2] else \"(\" + item[2].replace(\"None, \", \"\").replace(\"None,\", \"\").replace(\", None\", \"\").replace(\",None\", \"\") + \", True, 0, True)\"\n",
    "            s = s.replace(\"\".join(item),item[0] + \".str.contains\" + item_2_adj.replace('\",\"', \"|\").replace('\", \"', \"|\"))\n",
    "        return s\n",
    "\n",
    "    def adjust_sum(self, s):\n",
    "        \"\"\"Adjust SUM by adding additional parenthesis\"\"\" \n",
    "        for item in re.findall(r\"(SUM\\s?\\()(\\(?.*\\).*?\\)?)\", s):\n",
    "            s = s.replace(\"\".join(item),item[0] + \"(\" + item[1] + \")\")\n",
    "        return s\n",
    "\n",
    "    def preprocess_pattern(self, pattern):\n",
    "        # Pattern: AND, OR\n",
    "        pattern = self.replace_and_or(pattern)\n",
    "        # Pattern: SUBSTR\n",
    "        pattern = self.replace_substr(pattern)\n",
    "        # Pattern: IN, NOT IN\n",
    "        pattern = self.replace_in_not_in(pattern)\n",
    "        # Pattern: SUM\n",
    "        pattern = self.adjust_sum(pattern)\n",
    "\n",
    "        return pattern\n",
    "\n",
    "    def make_pattern_expression(self, expression, name, parameters):\n",
    "        \"\"\"Make expressions for the miner\"\"\"\n",
    "        parameters['solvency'] = True\n",
    "        pandas_expressions = data_patterns.to_pandas_expressions(expression, {}, parameters, None)\n",
    "        pattern = [[name, 0] + [expression] + [0, 0, 0] + [\"DNB\"] + [{}] + pandas_expressions + [\"\", \"\", \"\"]]\n",
    "        return pattern\n",
    "\n",
    "    def pre_process_rules(self, df_rules):\n",
    "        \"\"\"Transform rules so that we can evaluate them. Not all rules are fit to be evaluated\"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        df_rules['datapoints'] = ''\n",
    "        df_rules['datapoints'] = df_rules['datapoints'].astype('object')\n",
    "        df_rules['templates'] = ''\n",
    "        df_rules['templates'] = df_rules['templates'].astype('object')\n",
    "        for row in df_rules.index:\n",
    "            rule_original = df_rules.loc[row, 'Formule']\n",
    "            if not isinstance(rule_original, str):\n",
    "                logger.info(\"Rule \" + row + \": \" + \"duplicate rule. \")\n",
    "                rule_original = rule_original.values[0]\n",
    "            rule_original, datapoints = self.datapoints2pandas(rule_original)\n",
    "            df_rules.at[row, 'datapoints'] = datapoints\n",
    "            df_rules.at[row, 'templates'] = list(set([datapoint[0:13].upper() for datapoint in datapoints]))\n",
    "            df_rules.loc[row, 'Formule_input'] = rule_original\n",
    "\n",
    "        df_rules['error'] = ''  # error message\n",
    "        df_rules['n_patterns'] = 0  # number of patterns derived from rules\n",
    "\n",
    "        return df_rules\n",
    "\n",
    "    def unpack_rows_columns(self, row_range, column_range, datapoints, df_datapoints):\n",
    "        \"Unpack rows and columns\"\n",
    "        datapoints_not_found = []\n",
    "        expansion_dict = {}\n",
    "        # are the datapoints in the rule in the instance?\n",
    "        for datapoint in datapoints:\n",
    "            if datapoint not in self.entrypoint_datapoints:  # if datapoint is not there, see if we need to add rows or columns\n",
    "                new_list = []\n",
    "                bool_wildcard = \",#\" in datapoint\n",
    "                datapoint_orig = datapoint\n",
    "                datapoint = datapoint.replace(\",#\", \"\")\n",
    "                if datapoint[14] == \"C\" and (row_range[0] != \"\" or row_range[0].upper() == \"ALL\"):\n",
    "                    if len(row_range) == 1 and row_range[0].upper() == \"ALL\":\n",
    "                        for col in self.entrypoint_datapoints:\n",
    "                            reg = re.search(datapoint[0:14] + \"R....,\" + datapoint[14:],col)  # do for all rows if necessary\n",
    "                            if reg:\n",
    "                                new_list.append(reg.group(0))\n",
    "                    else:\n",
    "                        rows = []\n",
    "                        for r in row_range:\n",
    "                            if len(r) - len(r.replace(\"-\", \"\")) == 1:  # range\n",
    "                                low, high = r.split(\"-\")\n",
    "                                rows.extend(list(df_datapoints[(df_datapoints['tabelcode'] == datapoint[0:13]) &\n",
    "                                                                (df_datapoints['kolom'] == datapoint[14:]) &\n",
    "                                                                (df_datapoints['rij'].str[-4:] >= low) &\n",
    "                                                                (df_datapoints['rij'].str[-4:] <= high)\n",
    "                                                                ].rij))\n",
    "                            else:\n",
    "                                if r.upper()[0] == 'R':\n",
    "                                    rows.extend([r.upper()])\n",
    "                                else:\n",
    "                                    rows.extend([('R' + r)])\n",
    "                        for r in rows:\n",
    "                            new_list.append(datapoint[0:14] + r + \",\" + datapoint[14:])\n",
    "                if datapoint[14] == \"R\" and (column_range[0] != \"\" or column_range[0].upper() == \"ALL\"):\n",
    "                    if len(column_range) == 1 and column_range[0].upper() == \"ALL\":\n",
    "                        for col in self.entrypoint_datapoints:\n",
    "                            reg = re.search(datapoint + \",C....\", col)  # do for all columns if necessary\n",
    "                            if reg:\n",
    "                                new_list.append(reg.group(0))\n",
    "                    else:\n",
    "                        cols = []\n",
    "                        for c in column_range:\n",
    "                            if len(c) - len(c.replace(\"-\", \"\")) == 1:  # range\n",
    "                                low, high = c.split(\"-\")\n",
    "                                cols.extend(list(df_datapoints[(df_datapoints['tabelcode'] == datapoint[0:13]) &\n",
    "                                                                (df_datapoints['rij'] == datapoint[14:]) &\n",
    "                                                                (df_datapoints['kolom'].str[-4:] >= low) &\n",
    "                                                                (df_datapoints['kolom'].str[-4:] <= high)\n",
    "                                                                ].kolom))\n",
    "                            else:\n",
    "                                if c.upper()[0] == 'C':\n",
    "                                    cols.extend([c.upper()])\n",
    "                                else:\n",
    "                                    cols.extend([('C' + c)])\n",
    "                        for c in cols:\n",
    "                            new_list.append(datapoint + \",\" + c)\n",
    "                if new_list != []:\n",
    "                    # Wildcard # notation indicates that we need to sum over all the datapoints\n",
    "                    new_list = ['\"},{\"'.join(new_list)] if bool_wildcard else new_list\n",
    "                    expansion_dict[datapoint_orig] = new_list\n",
    "                else:\n",
    "                    datapoints_not_found.append(datapoint_orig)\n",
    "\n",
    "        return expansion_dict, datapoints_not_found\n",
    "\n",
    "    def process_rule(self, pre_expression, name, datapoints, expansion_dict, df_datapoints, parameters):\n",
    "        \"\"\"Some rules have multiple rows or columns. This function makes all the expressions with every row/column\"\"\"\n",
    "        expressions = []\n",
    "        invalid_expressions = \"\"\n",
    "        if expansion_dict:\n",
    "            if datapoints[0] in expansion_dict.keys():\n",
    "                zero = datapoints[0]\n",
    "            else:\n",
    "                zero = datapoints[1]\n",
    "            bool_wildcard = \",#\" in pre_expression\n",
    "            for i in range(len(expansion_dict[zero])):\n",
    "                expression = pre_expression\n",
    "                valid_expression = True\n",
    "                for datapoint in datapoints:\n",
    "                    if datapoint in expansion_dict.keys():\n",
    "                        datapoints_wildcard = [item for item in re.findall(r\"(S\\.\\d\\d\\.\\d\\d\\.\\d\\d\\.\\d\\d,R\\d\\d\\d\\d,C\\d\\d\\d\\d)*\", expansion_dict[datapoint][i]) if item != '']\n",
    "                        for datapoint_wildcard in datapoints_wildcard:\n",
    "                            if len(df_datapoints[(df_datapoints['tabelcode'] == datapoint_wildcard[:13]) &\n",
    "                                                (df_datapoints['rij'] == datapoint_wildcard[14:19].upper()) &\n",
    "                                                (df_datapoints['kolom'] == datapoint_wildcard[20:25].upper())]) == 0:\n",
    "                                valid_expression = False\n",
    "                        expression = expression.replace(datapoint, expansion_dict[datapoint][i])\n",
    "                if valid_expression:\n",
    "                    expressions.extend(self.make_pattern_expression(expression, name, parameters))\n",
    "                else:\n",
    "                    if invalid_expressions == \"\":\n",
    "                        invalid_expressions = invalid_expressions + \"(\" + expression + \")\"\n",
    "                    else:\n",
    "                        invalid_expressions = invalid_expressions + \", (\" + expression + \")\"\n",
    "        else:\n",
    "            expressions.extend(self.make_pattern_expression(pre_expression, name, parameters))\n",
    "\n",
    "        return expressions, invalid_expressions\n",
    "\n",
    "    def process_rules(self, df_datapoints, parameters):\n",
    "        \"\"\"Evaluate all rules and stores the result in df_rules\"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        rules_expressions = []\n",
    "        for idx in range(len(self.df_rules.index)):\n",
    "            row = self.df_rules.index[idx]\n",
    "            rule_original = self.df_rules.loc[row, 'Formule_input']\n",
    "            rule_name = self.df_rules.index[idx]\n",
    "            datapoints = self.df_rules.loc[row, 'datapoints'].copy()\n",
    "            templates = self.df_rules.loc[row, 'templates']\n",
    "            self.df_rules['Rijen'] = self.df_rules['Rijen'].astype(str)\n",
    "            self.df_rules['Kolommen'] = self.df_rules['Kolommen'].astype(str)\n",
    "            row_range = self.df_rules.loc[row, 'Rijen'].replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \";\").split(\";\")\n",
    "            column_range = self.df_rules.loc[row, 'Kolommen'].replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \";\").split(\";\")\n",
    "            # are the templates in the rule in the instance?\n",
    "            templates_not_found = []\n",
    "            for template in templates:\n",
    "                if template not in self.entrypoint_templates:\n",
    "                    templates_not_found.append(template)\n",
    "\n",
    "            if templates_not_found == []:\n",
    "                expansion_dict, datapoints_not_found = self.unpack_rows_columns(row_range, column_range, datapoints, df_datapoints)\n",
    "                if datapoints_not_found == []:\n",
    "                    rule_expressions, invalid_expressions = self.process_rule(rule_original, rule_name, datapoints, expansion_dict, df_datapoints, parameters)\n",
    "                    rules_expressions.extend(rule_expressions)\n",
    "                    if invalid_expressions != \"\":\n",
    "                        self.df_rules.loc[row, 'error'] = \\\n",
    "                            'Some expressions skipped due to invalid datapoint references: ' + invalid_expressions\n",
    "                        logger.warning(\"Rule \" + row + \", \" + self.df_rules.loc[row, 'error'])\n",
    "                    else:\n",
    "                        self.df_rules.loc[row, 'error'] = ''\n",
    "                    self.df_rules.loc[row, 'n_patterns'] = len(rule_expressions)\n",
    "                    logger.info(\"Rule \" + row + \", \" + str(len(rule_expressions)) + \" pattern(s) generated\")\n",
    "                else:\n",
    "                    # expression = rule_original\n",
    "                    self.df_rules.loc[row, 'error'] = 'missing datapoint(s): ' + str(datapoints_not_found)\n",
    "                    logger.warning(\"Rule \" + row + \", \" + self.df_rules.loc[row, 'error'])\n",
    "            else:\n",
    "                # expression = rule_original\n",
    "                self.df_rules.loc[row, 'error'] = 'missing template(s): ' + str(templates_not_found)\n",
    "                logger.warning(\"Rule \" + row + \", \" + self.df_rules.loc[row, 'error'])\n",
    "\n",
    "        df_patterns = pd.DataFrame(data = rules_expressions, columns = data_patterns.PATTERNS_COLUMNS)\n",
    "        df_patterns.index.name = 'index'\n",
    "\n",
    "        return df_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_qrs = Evaluator(df_rules_qrs, df_datapoints, PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_qrs.df_patterns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_ars = Evaluator(df_rules_ars, df_datapoints, PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_ars.df_patterns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export patterns to rules folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_qrs.df_patterns.to_excel(join('..', 'solvency2-rules', \"qrs_patterns_additional_rules.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_ars.df_patterns.to_excel(join('..', 'solvency2-rules', \"ars_patterns_additional_rules.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqr_eva2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "21b176772a6c742f14e767e147c714614e0065ff528d0fa105ef1e84b18d9160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
