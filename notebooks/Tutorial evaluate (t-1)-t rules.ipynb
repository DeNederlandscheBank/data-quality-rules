{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial describes how to evaluate rules that are applicable to two consecutive periods (year and quarter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arelle import ModelManager, Cntlr, ModelFormulaObject, ModelXbrl, ViewFileFormulae, XbrlConst, ViewFileRenderedGrid\n",
    "from arelle import RenderingEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import pickle\n",
    "import re\n",
    "from src import Evaluator\n",
    "import logging\n",
    "import data_patterns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECIMALS = 0\n",
    "RULES_PATH = join('..', 'solvency2-rules')\n",
    "INSTANCES_DATA_PATH = join('..','data','instances')\n",
    "DATAPOINTS_PATH = join('..', 'data', 'datapoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing the (t-1)-t rules that are applicable to two consecutive periods. We import a set of rules used to evaluate year data and a set of rules for quarter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S2_betweenperiods_ARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_ARS = pd.read_excel(join(RULES_PATH,'S2_betweenperiods_ARS.xlsx'), engine='openpyxl')\n",
    "\n",
    "#Capitalize row-column references:\n",
    "column_replace = set([column for sublist in [row for row in dfr_ARS['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_ARS.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)\n",
    "column_replace = set([column for sublist in [row for row in dfr_ARS['pandas ex'].str.findall(r'r\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_ARS.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)\n",
    "dfr_ARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S2_betweenperiods_QRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_QRS = pd.read_excel(join(RULES_PATH,'S2_betweenperiods_QRS.xlsx'), engine='openpyxl')\n",
    "\n",
    "#Capitalize row-column references:\n",
    "column_replace = set([column for sublist in [row for row in dfr_QRS['pandas ex'].str.findall(r'c\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_QRS.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)\n",
    "column_replace = set([column for sublist in [row for row in dfr_QRS['pandas ex'].str.findall(r'r\\d\\d\\d\\d')] for column in sublist])\n",
    "for ref in column_replace:\n",
    "    dfr_QRS.replace(to_replace=ref, value=ref.capitalize(), inplace=True, regex=True)\n",
    "dfr_QRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the reporting data. We import the data of two consecutive periods. In the tutorial 'Convert XBRL-instances to CSV, HTML and pickles' the XBRL-instances are converted to pickle files per template. The pickle files are written to the data/instances folder. The rules are applicable to all tables with closed axis. We import these pickle files. When comparing two periods it can be the case that two different taxonomies are applicable. The right taxonomy has to be selected in the tutorial 'Convert XBRL-instances to CSV, HTML and pickles' to convert the XBRL-instance properly. \n",
    "\n",
    "The list _instances_ARS_ contains the names of the folders with the converted XBRL-instance for yearly data. The list _instances_QRS_ contains the names of the folders with the converted XBRL-instance for two consecutive quarters. Finally, we also have to define the category of the insurer. The rules are set-up for each type of insurer separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_ARS = [\"ars_260_instance\", \"ars_270_instance\"]\n",
    "instances_QRS = [\"qrs_260_instance\", \"qrs_270_instance\"]\n",
    "categorie = 'Schade' #which type of insurer the instance belongs to (Schade, Herverzekeraar, Leven)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S2_betweenperiods_ARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapoints = pd.read_csv(join(DATAPOINTS_PATH, 'ARS.csv'), sep=\";\").fillna(\"\")  # load file to dataframe\n",
    "dft = pd.DataFrame()\n",
    "for instance in instances_ARS:\n",
    "    df_closed_axis = pd.DataFrame()\n",
    "    tables_closed_axis = []  # for listing all input tables with closed axis\n",
    "    tables_complete_set = df_datapoints.tabelcode.sort_values().unique().tolist()  # list of all ARS tables\n",
    "    tables = [table for table in tables_complete_set \n",
    "        if isfile(join(INSTANCES_DATA_PATH, instance, table + '.pickle'))]  # ARS tables found in the specified instance path\n",
    "    for table in [table for table in tables if table not in ['S.14.01.01.04','S.30.03.01.01']]:  #tables:\n",
    "        if isfile(join(INSTANCES_DATA_PATH, instance, table + '.pickle')):\n",
    "            df = pd.read_pickle(join(INSTANCES_DATA_PATH,instance, table + '.pickle'))  # read dataframe\n",
    "        else:\n",
    "            continue   \n",
    "        if df.index.nlevels > 2:  # if more than 2 indexes (entity, period), then the table has an open axis\n",
    "            continue\n",
    "        else:  # closed axis\n",
    "            tables_closed_axis.append(table)  # add to relevant list\n",
    "            # Add table to dataframe with all data from closed axis tables\n",
    "            if len(df_closed_axis) == 0:  # no data yet --> copy dataframe\n",
    "                df_closed_axis = df.copy()\n",
    "            else:  # join to existing dataframe\n",
    "                df_closed_axis = df_closed_axis.join(df)\n",
    "    if len(dft) == 0:  # no data yet \n",
    "        dft = df_closed_axis\n",
    "    else:  # join to existing dataframe\n",
    "        dft=dft.append(df_closed_axis)\n",
    "dft = dft.reset_index()\n",
    "dft['categorie'] = categorie\n",
    "numerical_columns = ['entity','period','categorie'] + [dft.columns[c] for c in range(len(dft.columns))\n",
    "                        if ((dft.dtypes[c] == 'float64') or (dft.dtypes[c] == 'int64'))] #select only numerical columns\n",
    "df_ARS = dft[numerical_columns]\n",
    "df_ARS['period'] = df_ARS['period'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d')) #convert to datetime\n",
    "df_ARS.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we work with dummy data in order to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(join('..','tests','data','demo','ARS.pkl'), 'rb') as handle:\n",
    "#     df_ARS = pickle.load(handle)\n",
    "# df_ARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S2_betweenperiods_QRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapoints = pd.read_csv(join(DATAPOINTS_PATH, 'QRS.csv'), sep=\";\").fillna(\"\")  # load file to dataframe\n",
    "dft = pd.DataFrame()\n",
    "for instance in instances_QRS:\n",
    "    df_closed_axis = pd.DataFrame()\n",
    "    tables_closed_axis = []  # for listing all input tables with closed axis\n",
    "    # get tables\n",
    "    tables_complete_set = df_datapoints.tabelcode.sort_values().unique().tolist()  # list of all QRS tables\n",
    "    tables = [table for table in tables_complete_set \n",
    "        if isfile(join(INSTANCES_DATA_PATH,instance,table + '.pickle'))]  # QRS tables found in the specified INSTANCES_DATA_PATH\n",
    "    for table in [table for table in tables if table not in ['S.14.01.01.04','S.30.03.01.01']]:  #tables:\n",
    "        if isfile(join(INSTANCES_DATA_PATH,instance, table + '.pickle')):\n",
    "            df = pd.read_pickle(join(INSTANCES_DATA_PATH,instance, table + '.pickle'))  # read dataframe\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        if df.index.nlevels > 2:  # if more than 2 indexes (entity, period), then the table has an open axis\n",
    "            continue\n",
    "        else:  # closed axis\n",
    "            tables_closed_axis.append(table)  # add to relevant list\n",
    "        \n",
    "            # Add table to dataframe with all data from closed axis tables\n",
    "            if len(df_closed_axis) == 0:  # no data yet --> copy dataframe\n",
    "                df_closed_axis = df.copy()\n",
    "            else:  # join to existing dataframe\n",
    "                df_closed_axis = df_closed_axis.join(df)\n",
    "    if len(dft) == 0:  # no data yet \n",
    "        dft = df_closed_axis\n",
    "    else:  # join to existing dataframe\n",
    "        dft=dft.append(df_closed_axis)\n",
    "dft=dft.reset_index()\n",
    "dft['categorie']='Schade'\n",
    "numerical_columns = ['entity','period','categorie'] + [dft.columns[c] for c in range(len(dft.columns))\n",
    "                        if ((dft.dtypes[c] == 'float64') or (dft.dtypes[c] == 'int64'))]\n",
    "df_QRS = dft[numerical_columns]\n",
    "df_QRS['period']=df_QRS['period'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d')) #convert to datetime\n",
    "df_QRS.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we work with dummy data in order to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(join('..','tests','data','demo','QRS.pkl'), 'rb') as handle:\n",
    "#     df_QRS = pickle.load(handle)\n",
    "# df_QRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate S2_betweenperiods_ARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_ARS)\n",
    "miner.df_data = df_ARS\n",
    "miner.metapatterns = {'cluster': 'categorie'}\n",
    "miner.convert_to_time(['entity', 'categorie'], 'period')\n",
    "miner.df_data = miner.df_data.reset_index()\n",
    "\n",
    "results = miner.analyze()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S2_betweenperiods_QRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=dfr_QRS)\n",
    "miner.df_data = df_QRS\n",
    "miner.metapatterns = {'cluster':'categorie'}\n",
    "miner.convert_to_time(['entity', 'categorie'], 'period', set_year=False)\n",
    "miner.df_data = miner.df_data.reset_index()\n",
    "\n",
    "results = miner.analyze()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqr_eva2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "21b176772a6c742f14e767e147c714614e0065ff528d0fa105ef1e84b18d9160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
