{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Evaluate DNBs additional Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a tutorial for the evaluation of DNBs additional Rules for the following Solvency II reports:\n",
    "- Annual Reporting Solo (ARS); and\n",
    "- Quarterly Reporting Solo (QRS)\n",
    "\n",
    "Besides the necessary preparation, the tutorial consists of 6 steps:\n",
    "1. Read possible datapoints\n",
    "2. Read data\n",
    "3. Clean data\n",
    "4. Read additional rules\n",
    "5. Evaluate rules\n",
    "6. Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # dataframes\n",
    "import numpy as np  # mathematical functions, arrays and matrices\n",
    "from os.path import join, isfile  # some os dependent functionality\n",
    "import data_patterns  # evaluation of patterns\n",
    "import regex as re  # regular expressions\n",
    "from pprint import pprint  # pretty print\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRYPOINT: 'ARS' for 'Annual Reporting Solo' or 'QRS' for 'Quarterly Reporting Solo'\n",
    "# INSTANCE: Name of the report you want to evaluate the additional rules for\n",
    "\n",
    "ENTRYPOINT = 'ARS'  \n",
    "INSTANCE = 'ars_260_instance'  # Test instances: ars_260_instance or qrs_260_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAPOINTS_PATH: path to the excel-file containing all possible datapoints (simplified taxonomy)\n",
    "# RULES_PATH: path to the excel-file with the additional rules\n",
    "# INSTANCES_DATA_PATH: path to the source data\n",
    "# RESULTS_PATH: path to the results\n",
    "\n",
    "DATAPOINTS_PATH = join('..', 'data', 'datapoints')\n",
    "RULES_PATH = join('..', 'solvency2-rules')\n",
    "INSTANCES_DATA_PATH = join('..', 'data', 'instances', INSTANCE)\n",
    "RESULTS_PATH = join('..', 'results') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We log to rules.log in the data/instances path\n",
    "\n",
    "logging.basicConfig(filename = join(INSTANCES_DATA_PATH, 'rules.log'),level = logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read possible datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data/datapoints directory there is a file for both ARS and QRS in which all possible datapoints are listed (simplified taxonomy).  \n",
    "We will use this information to add all unreported datapoints to the imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapoints = pd.read_csv(join(DATAPOINTS_PATH, ENTRYPOINT.upper() + '.csv'), sep=\";\").fillna(\"\")  # load file to dataframe\n",
    "df_datapoints.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish 2 types of tables: \n",
    "- With a closed-axis, e.g. the balance sheet: an entity reports only 1 balance sheet per period\n",
    "- With an open-axis, e.g. the list of assets: an entity reports several 'rows of data' in the relevant table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we gather some general information:\n",
    "- A list of all possible reported tables\n",
    "- A list of all reported tables\n",
    "- A list of all tables that have not been reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_complete_set = df_datapoints.tabelcode.sort_values().unique().tolist()\n",
    "tables_reported = [table for table in tables_complete_set if isfile(join(INSTANCES_DATA_PATH, table + '.pickle'))]\n",
    "tables_not_reported = [table for table in tables_complete_set if table not in tables_reported]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides all separate tables, the 'Tutorial Convert XBRL-instance to CSV, HTML and pickles' also outputs a large dataframe with the data from all closed-axis tables combined.  \n",
    "We use this dataframe for evaluating the patterns on closed-axis tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_closed_axis = pd.read_pickle(join(INSTANCES_DATA_PATH, INSTANCE + '.pickle'))\n",
    "tables_closed_axis = sorted(list(set(x[:13] for x in df_closed_axis.columns)))\n",
    "df_closed_axis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-axis tables we create a dictionary with all data per table.  \n",
    "Later we will evaluate the additional rules on each seperate table in this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_open_axis = {}\n",
    "tables_open_axis = [table for table in tables_reported if table not in tables_closed_axis]\n",
    "\n",
    "for table in tables_open_axis:\n",
    "    df = pd.read_pickle(join(INSTANCES_DATA_PATH, table + '.pickle'))\n",
    "    \n",
    "    # Identify which columns within the open-axis table make a table row unique (index-columns):\n",
    "    index_columns_open_axis = [col for col in list(df.index.names) if col not in ['entity','period']]\n",
    "    \n",
    "    # Duplicate index-columns to data columns:\n",
    "    df.reset_index(level=index_columns_open_axis, inplace=True)\n",
    "    for i in range(len(index_columns_open_axis)):\n",
    "        df['index_col_' + str(i)] = df[index_columns_open_axis[i]].astype(str)\n",
    "        df.set_index(['index_col_' + str(i)], append=True, inplace=True)\n",
    "        \n",
    "    dict_open_axis[table] = df \n",
    "\n",
    "print(\"Open-axis tables:\")\n",
    "print(list(dict_open_axis.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to make 2 modifications on the data:\n",
    "1. Add unreported datapoints  \n",
    "so rules (partly) pointing to unreported datapoints can still be evaluated\n",
    "2. Change string values to uppercase  \n",
    "because the additional rules are defined using capital letters for textual comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datapoints = [x.replace(',,',',') for x in \n",
    "                  list(df_datapoints['tabelcode'] + ',' + df_datapoints['rij'] + ',' + df_datapoints['kolom'])]\n",
    "all_datapoints_closed = [x for x in all_datapoints if x.split(\",\")[0] in tables_closed_axis]\n",
    "all_datapoints_open = [x for x in all_datapoints if x.split(\",\")[0] in tables_open_axis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-axis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add not reported datapoints to the dataframe with data from closed axis tables:\n",
    "for col in [column for column in all_datapoints_closed if column not in list(df_closed_axis.columns)]:\n",
    "    df_closed_axis[col] = np.nan\n",
    "df_closed_axis.fillna(0, inplace = True)\n",
    "\n",
    "# string values to uppercase\n",
    "df_closed_axis = df_closed_axis.applymap(lambda s:s.upper() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-axis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in [table for table in dict_open_axis.keys()]:\n",
    "    all_datapoints_table = [x for x in all_datapoints_open if x.split(\",\")[0] == table]\n",
    "    for col in [column for column in all_datapoints_table if column not in list(dict_open_axis[table].columns)]:\n",
    "        dict_open_axis[table][col] = np.nan\n",
    "    dict_open_axis[table].fillna(0, inplace = True)\n",
    "    \n",
    "    dict_open_axis[table] = dict_open_axis[table].applymap(lambda s:s.upper() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read additional rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNBs additional validation rules are published as an Excel file on the DNB statistics website.  \n",
    "We included the Excel file in the project under data/downloaded files.\n",
    "\n",
    "The rules are already converted to a syntax Python can interpret, using the notebook: 'Convert DNBs Additional Validation Rules to Patterns'.  \n",
    "In the next line of code we read these converted rules (patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patterns = pd.read_excel(join(RULES_PATH, ENTRYPOINT.lower() + '_patterns_additional_rules.xlsx'), engine='openpyxl').fillna(\"\").set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-axis tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to evaluate the rules for closed-axis tables, we need to filter out:\n",
    "- patterns for open-axis tables; and\n",
    "- patterns pointing to tables that are not reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patterns_closed_axis = df_patterns.copy()\n",
    "df_patterns_closed_axis = df_patterns_closed_axis[df_patterns_closed_axis['pandas ex'].apply(\n",
    "    lambda expr: not any(table in expr for table in tables_not_reported) \n",
    "    and not any(table in expr for table in tables_open_axis))]\n",
    "df_patterns_closed_axis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have:\n",
    "- the data for closed-axis tables in a dataframe;\n",
    "- the patterns for closed-axis tables in a dataframe.\n",
    "\n",
    "To evaluate the patterns we need to create a 'PatternMiner' (part of the data_patterns package), and run the analyze function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "miner = data_patterns.PatternMiner(df_patterns=df_patterns_closed_axis)\n",
    "df_results_closed_axis = miner.analyze(df_closed_axis)\n",
    "df_results_closed_axis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-axis tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First find the patterns defined for open-axis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patterns_open_axis = df_patterns.copy()\n",
    "df_patterns_open_axis = df_patterns_open_axis[df_patterns_open_axis['pandas ex'].apply(\n",
    "    lambda expr: any(table in expr for table in tables_open_axis))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patterns involving multiple open-axis tables are not yet supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patterns_open_axis = df_patterns_open_axis[df_patterns_open_axis['pandas ex'].apply(\n",
    "    lambda expr: len(set(re.findall('S.\\d\\d.\\d\\d.\\d\\d.\\d\\d|T\\d[A-Z]?',expr)))) == 1]\n",
    "df_patterns_open_axis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we loop through the open-axis tables and evaluate the corresponding patterns on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_open_axis = {}  # dictionary with input and results per table\n",
    "for table in tables_open_axis:  # loop through open-axis tables\n",
    "    if df_patterns_open_axis['pandas ex'].apply(lambda expr: table in expr).sum() > 0:  # check if there are patterns\n",
    "        info = {}\n",
    "        info['data'] = dict_open_axis[table]  # select data\n",
    "        info['patterns'] = df_patterns_open_axis[df_patterns_open_axis['pandas ex'].apply(\n",
    "            lambda expr: table in expr)]  # select patterns\n",
    "        miner = data_patterns.PatternMiner(df_patterns=info['patterns'])\n",
    "        info['results'] = miner.analyze(info['data'])  # evaluate patterns\n",
    "        output_open_axis[table] = info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the first table (if there are rules for tables with an open axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(output_open_axis.keys()) > 0:\n",
    "    display(output_open_axis[list(output_open_axis.keys())[0]]['results'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine results for closed- and open-axis tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To output the results in a single file, we want to combine the results for closed-axis and open-axis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform results for open-axis tables, so it can be appended to results for closed-axis tables\n",
    "# The 'extra' index columns are converted to data columns\n",
    "def transform_results_open_axis(df):\n",
    "    if df.index.nlevels > 2:\n",
    "        reset_index_levels = list(range(2, df.index.nlevels))\n",
    "        df = df.reset_index(level=reset_index_levels)\n",
    "        rename_columns={}\n",
    "        for x in reset_index_levels:\n",
    "            rename_columns['level_' + str(x)] = 'id_column_' + str(x - 1)\n",
    "        df.rename(columns=rename_columns, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results_closed_axis.copy()  # results for closed axis tables\n",
    "for table in list(output_open_axis.keys()):  # for all open axis tables with rules -> append and sort results\n",
    "    df_results = transform_results_open_axis(output_open_axis[table]['results']).append(df_results, sort=False).sort_values(by=['pattern_id']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change column order so the dataframe starts with the identifying columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_order = []\n",
    "for i in range(1, len([col for col in list(df_results.columns) if col[:10] == 'id_column_']) + 1):\n",
    "    list_col_order.append('id_column_' + str(i))\n",
    "list_col_order.extend(col for col in list(df_results.columns) if col not in list_col_order)\n",
    "df_results = df_results[list_col_order]\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe df_results contains all output of the evaluation of the validation rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save all results use df_results\n",
    "# To save all exceptions use df_results['result_type']==False \n",
    "# To save all confirmations use df_results['result_type']==True\n",
    "\n",
    "# Here we save only the exceptions to the validation rules\n",
    "df_results[df_results['result_type']==False].to_excel(join(RESULTS_PATH, \"results.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of an error in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pandas code from the first pattern and evaluate it\n",
    "s = df_patterns.loc[4, 'pandas ex'].replace('df', 'df_closed_axis')\n",
    "print('Pattern:', s)\n",
    "display(eval(s)[re.findall('S.\\d\\d.\\d\\d.\\d\\d.\\d\\d,R\\d\\d\\d\\d,C\\d\\d\\d\\d|T\\d[A-Z]?,R\\d\\d\\d,C\\d\\d\\d',s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqr_eva2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "21b176772a6c742f14e767e147c714614e0065ff528d0fa105ef1e84b18d9160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
